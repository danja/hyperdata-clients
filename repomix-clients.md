This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document.
Generated by Repomix on: 2025-03-28T00:10:33.481Z

================================================================
File Summary
================================================================

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:

1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
   a. A separator line (================)
   b. The file path (File: path/to/file)
   c. Another separator line
   d. The full contents of the file
   e. A blank line

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: \*_/_
- Files matching these patterns are excluded: docs, data, docs/jsdoc, .nyc*output, .env, \*\*/*_, node_modules, _.log, **/_repomix_.txt, **/_.html, \*\*/data/_, **/\*copy.js, **/conversations.json
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

## Additional Info:

## User Provided Header:

clients repo

================================================================
Directory Structure
================================================================
examples/
basic.js
mcp.js
src/
common/
APIClient.js
ClientFactory.js
KeyManager.js
providers/
Claude.js
Groq.js
HuggingFace.js
MCPClient.js
Mistral.js
Ollama.js
OpenAIClient.js
Perplexity.js
tests/
helpers/
reporter.js
integration/
mcp-integration.spec.js
unit/
base-client.spec.js
claude-2.spec.js
claude.spec.js
factory-mcp.spec.js
factory.spec.js
groq.spec.js
huggingface.spec.js
mcp-client.spec.js
mcp-persistence.spec.js
mistral.spec.js
ollama.spec.js
openai.spec.js
perplexity.spec.js
.gitignore
clients.d.ts
env-example.txt
github-workflow (1).txt
github-workflow.txt
index.d.ts.ts
jasmine.json
package.json
readme.md
README.md
repomix.config.json
test-env.txt
types-package.json

================================================================
Files
================================================================

================
File: examples/basic.js
================
import { createAPIClient } from '../common/ClientFactory.js';

const client = createAPIClient('openai', { apiKey: 'your-key' });
const response = await client.chat([
{ role: 'user', content: 'Hello!' } // should be using .env
]);

================
File: examples/mcp.js
================
const client = await createAPIClient('openai', {
mcp: {
resources: {
'docs': {
uri: 'file://docs/',
mimeType: 'text/markdown'
}
},
tools: {
'search': {
name: 'Search Documentation',
description: 'Search through documentation',
execute: async (query) => { /_ ... _/ }
}
},
prompts: {
'summarize': {
name: 'Summarize Text',
template: (context) => `Summarize: ${context.text}`
}
}
}
});

// Use MCP features
const resource = await client.getResource('docs');
const result = await client.executeTool('search', { query: 'api' });
const prompt = await client.renderPrompt('summarize', { text: 'content' });

================
File: src/common/APIClient.js
================
export class APIClient {
constructor(config = {}) {
if (this.constructor === APIClient) {
throw new Error('Cannot instantiate abstract class');
}
this.config = config;
}

    async chat(messages, options = {}) {
        throw new Error('Method chat() must be implemented');
    }

    async complete(prompt, options = {}) {
        throw new Error('Method complete() must be implemented');
    }

    async embedding(text, options = {}) {
        throw new Error('Method embedding() must be implemented');
    }

    async stream(messages, callback, options = {}) {
        throw new Error('Method stream() must be implemented');
    }

}

export class APIError extends Error {
constructor(message, provider, code) {
super(message);
this.name = 'APIError';
this.provider = provider;
this.code = code;
}
}

================
File: src/common/ClientFactory.js
================
import { OpenAIClient } from '../providers/OpenAIClient.js'
import { Claude } from '../providers/Claude.js'
import { Ollama } from '../providers/Ollama.js'
import { Mistral } from '../providers/Mistral.js'
import { Groq } from '../providers/Groq.js'
import { Perplexity } from '../providers/Perplexity.js'
import { HuggingFace } from '../providers/HuggingFace.js'
import { MCPClient } from '../providers/MCPClient.js'
import { KeyManager } from '../common/KeyManager.js'

const PROVIDERS = {
openai: OpenAIClient,
claude: Claude,
ollama: Ollama,
mistral: Mistral,
groq: Groq,
perplexity: Perplexity,
huggingface: HuggingFace
}

export async function createAPIClient(provider, config = {}) {
const ClientClass = PROVIDERS[provider.toLowerCase()]
if (!ClientClass) {
throw new Error(`Unknown AI provider: ${provider}`)
}

    // Validate and get API key
    const key = KeyManager.getKey(config, provider)

    // Create base client
    const client = new ClientClass({ ...config, apiKey: key })

    // Wrap with MCP if requested
    if (config.mcp) {
        const mcpClient = new MCPClient(config.mcp)

        // Register MCP resources if provided
        if (config.mcp.resources) {
            for (const [id, resource] of Object.entries(config.mcp.resources)) {
                await mcpClient.registerResource(id, resource)
            }
        }

        // Register MCP tools if provided
        if (config.mcp.tools) {
            for (const [id, tool] of Object.entries(config.mcp.tools)) {
                await mcpClient.registerTool(id, tool)
            }
        }

        // Register MCP prompts if provided
        if (config.mcp.prompts) {
            for (const [id, prompt] of Object.entries(config.mcp.prompts)) {
                await mcpClient.registerPrompt(id, prompt)
            }
        }

        // Extend client with MCP capabilities
        return new Proxy(client, {
            get(target, prop) {
                if (prop in mcpClient) {
                    return mcpClient[prop].bind(mcpClient)
                }
                return target[prop]
            }
        })
    }

    return client

}

================
File: src/common/KeyManager.js
================
export class KeyManager {
static validateKey(key, provider) {
if (!key) {
throw new Error(`${provider} API key is required`);
}

        const patterns = {
            openai: /^sk-[a-zA-Z0-9]{32,}$/,
            claude: /^sk-ant-[a-zA-Z0-9]{32,}$/,
            mistral: /^[a-zA-Z0-9]{32,}$/,
            groq: /^gsk_[a-zA-Z0-9]{32,}$/,
            perplexity: /^pplx-[a-zA-Z0-9]{32,}$/,
            huggingface: /^hf_[a-zA-Z0-9]{32,}$/
        };

        if (patterns[provider] && !patterns[provider].test(key)) {
            throw new Error(`Invalid ${provider} API key format`);
        }
    }

    static getKey(config, provider) {
        const key = config.apiKey || process.env[`${provider.toUpperCase()}_API_KEY`];
        this.validateKey(key, provider);
        return key;
    }

    static rotateKey(config, provider, newKey) {
        this.validateKey(newKey, provider);
        const envVar = `${provider.toUpperCase()}_API_KEY`;
        process.env[envVar] = newKey;
        return newKey;
    }

}

================
File: src/providers/Claude.js
================
import Anthropic from '@anthropic-ai/sdk'
import { APIClient, APIError } from '../common/APIClient.js'

export class Claude extends APIClient {
constructor(config = {}) {
super(config)
const apiKey = config.apiKey || process.env.CLAUDE_API_KEY

        if (!apiKey) {
            throw new Error('Claude API key is required. Provide it in constructor or set CLAUDE_API_KEY environment variable.')
        }

        this.client = new Anthropic({
            apiKey,
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.messages.create({
                model: options.model || 'claude-3-opus-20240229',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.content[0].text
        } catch (error) {
            throw new APIError(error.message, 'claude', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.embeddings.create({
                model: options.model || 'claude-3-embedding',
                input: text instanceof Array ? text : [text],
                ...options
            })
            return response.embeddings[0]
        } catch (error) {
            throw new APIError(error.message, 'claude', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.messages.create({
                model: options.model || 'claude-3-opus-20240229',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.delta?.text || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new APIError(error.message, 'claude', error.status)
        }
    }

}

================
File: src/providers/Groq.js
================
import { Groq } from 'groq-sdk'
import { APIClient, APIError } from '../common/APIClient.js'

export class Groq extends APIClient {
constructor(config = {}) {
super(config)
const apiKey = config.apiKey || process.env.GROQ_API_KEY

        if (!apiKey) {
            throw new Error('Groq API key is required. Provide it in constructor or set GROQ_API_KEY environment variable.')
        }

        this.client = new Groq({
            apiKey,
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.completions.create({
                model: options.model || 'llama3-8b-8192',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new APIError(error.message, 'groq', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        throw new APIError('Embeddings not supported by Groq', 'groq', 'UNSUPPORTED_OPERATION')
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.completions.create({
                model: options.model || 'llama3-8b-8192',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new APIError(error.message, 'groq', error.status)
        }
    }

}

================
File: src/providers/HuggingFace.js
================
import { HfInference } from '@huggingface/inference'
import { APIClient, APIError } from '../common/APIClient.js'

export class HuggingFace extends APIClient {
constructor(config = {}) {
super(config)
const apiKey = config.apiKey || process.env.HUGGINGFACE_API_KEY

        if (!apiKey) {
            throw new Error('HuggingFace API key is required. Provide it in constructor or set HUGGINGFACE_API_KEY environment variable.')
        }

        this.client = new HfInference(apiKey)
    }

    async chat(messages, options = {}) {
        try {
            const input = messages.map(m => `${m.role}: ${m.content}`).join('\n')
            const response = await this.client.textGeneration({
                model: options.model || 'gpt2',
                inputs: input,
                parameters: {
                    max_new_tokens: options.maxTokens || 50,
                    temperature: options.temperature || 0.7,
                    ...options.parameters
                }
            })
            return response.generated_text
        } catch (error) {
            throw new APIError(error.message, 'huggingface', error.status)
        }
    }

    async complete(prompt, options = {}) {
        try {
            const response = await this.client.textGeneration({
                model: options.model || 'gpt2',
                inputs: prompt,
                parameters: {
                    max_new_tokens: options.maxTokens || 50,
                    temperature: options.temperature || 0.7,
                    ...options.parameters
                }
            })
            return response.generated_text
        } catch (error) {
            throw new APIError(error.message, 'huggingface', error.status)
        }
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.featureExtraction({
                model: options.model || 'sentence-transformers/all-MiniLM-L6-v2',
                inputs: text instanceof Array ? text : [text],
                ...options
            })
            return response[0]
        } catch (error) {
            throw new APIError(error.message, 'huggingface', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        throw new APIError('Streaming not supported by HuggingFace Inference API', 'huggingface', 'UNSUPPORTED_OPERATION')
    }

}

================
File: src/providers/MCPClient.js
================
import { APIClient, APIError } from '../common/APIClient.js'

export class MCPClient extends APIClient {
constructor(config = {}) {
super(config)
this.resources = new Map()
this.tools = new Map()
this.prompts = new Map()
}

    async registerResource(id, resource) {
        this.resources.set(id, {
            ...resource,
            uri: resource.uri || `mcp:resource:${id}`,
            mimeType: resource.mimeType || 'text/plain'
        })
    }

    async registerTool(id, tool) {
        this.tools.set(id, {
            ...tool,
            name: tool.name || id,
            description: tool.description || ''
        })
    }

    async registerPrompt(id, prompt) {
        this.prompts.set(id, {
            ...prompt,
            name: prompt.name || id,
            description: prompt.description || ''
        })
    }

    async getResource(id) {
        const resource = this.resources.get(id)
        if (!resource) throw new APIError(`Resource not found: ${id}`, 'mcp', 'RESOURCE_NOT_FOUND')
        return resource
    }

    async executeTool(id, args) {
        const tool = this.tools.get(id)
        if (!tool) throw new APIError(`Tool not found: ${id}`, 'mcp', 'TOOL_NOT_FOUND')
        if (!tool.execute) throw new APIError(`Tool not executable: ${id}`, 'mcp', 'TOOL_NOT_EXECUTABLE')
        return tool.execute(args)
    }

    async renderPrompt(id, context) {
        const prompt = this.prompts.get(id)
        if (!prompt) throw new APIError(`Prompt not found: ${id}`, 'mcp', 'PROMPT_NOT_FOUND')
        if (!prompt.template) throw new APIError(`Invalid prompt template: ${id}`, 'mcp', 'INVALID_PROMPT')
        return prompt.template(context)
    }

    describe() {
        return {
            resources: Array.from(this.resources.entries()).map(([id, r]) => ({
                id,
                uri: r.uri,
                mimeType: r.mimeType
            })),
            tools: Array.from(this.tools.entries()).map(([id, t]) => ({
                id,
                name: t.name,
                description: t.description
            })),
            prompts: Array.from(this.prompts.entries()).map(([id, p]) => ({
                id,
                name: p.name,
                description: p.description
            }))
        }
    }

}

================
File: src/providers/Mistral.js
================
import Mistral from '@mistralai/mistralai'
import { APIClient, APIError } from '../common/APIClient.js'

export class Mistral extends APIClient {
constructor(config = {}) {
super(config)
const apiKey = config.apiKey || process.env.MISTRAL_API_KEY

        if (!apiKey) {
            throw new Error('Mistral API key is required. Provide it in constructor or set MISTRAL_API_KEY environment variable.')
        }

        this.client = new Mistral({
            apiKey,
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.create({
                model: options.model || 'mistral-tiny',
                messages,
                maxTokens: options.maxTokens,
                temperature: options.temperature || 0.7,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new APIError(error.message, 'mistral', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.embeddings.create({
                model: options.model || 'mistral-embed',
                input: text instanceof Array ? text : [text],
                ...options
            })
            return response.data[0].embedding
        } catch (error) {
            throw new APIError(error.message, 'mistral', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.stream({
                model: options.model || 'mistral-tiny',
                messages,
                maxTokens: options.maxTokens,
                temperature: options.temperature || 0.7,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new APIError(error.message, 'mistral', error.status)
        }
    }

}

================
File: src/providers/Ollama.js
================
import ollama from 'ollama'
import { APIClient, APIError } from '../common/APIClient.js'

export class Ollama extends APIClient {
constructor(config = {}) {
super(config)
this.baseUrl = config.baseUrl || process.env.OLLAMA_HOST || 'http://localhost:11434'
}

    async chat(messages, options = {}) {
        try {
            const response = await ollama.chat({
                model: options.model || 'llama2',
                messages,
                ...options
            }, { baseUrl: this.baseUrl })
            return response.message.content
        } catch (error) {
            throw new APIError(error.message, 'ollama', error.code)
        }
    }

    async complete(prompt, options = {}) {
        try {
            const response = await ollama.generate({
                model: options.model || 'llama2',
                prompt,
                ...options
            }, { baseUrl: this.baseUrl })
            return response.response
        } catch (error) {
            throw new APIError(error.message, 'ollama', error.code)
        }
    }

    async embedding(text, options = {}) {
        try {
            const response = await ollama.embeddings({
                model: options.model || 'nomic-embed-text',
                prompt: text,
                ...options
            }, { baseUrl: this.baseUrl })
            return response.embedding
        } catch (error) {
            throw new APIError(error.message, 'ollama', error.code)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await ollama.chat({
                model: options.model || 'llama2',
                messages,
                stream: true,
                ...options
            }, { baseUrl: this.baseUrl })

            for await (const chunk of stream) {
                const content = chunk.message?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new APIError(error.message, 'ollama', error.code)
        }
    }

}

================
File: src/providers/OpenAIClient.js
================
// providers/openai.js
import OpenAI from 'openai'
import { APIClient, APIError } from '../common/APIClient.js'

export class OpenAIClient extends APIClient {
constructor(config = {}) {
super(config)
this.client = new OpenAI({
apiKey: config.apiKey || process.env.OPENAI_API_KEY,
...config.clientOptions
})

        if (!this.client.apiKey) {
            throw new Error('OpenAI API key is required. Provide it in constructor or set OPENAI_API_KEY environment variable.')
        }
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.completions.create({
                model: options.model || 'gpt-4-turbo-preview',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new APIError(error.message, 'openai', error.status)
        }
    }

    async complete(prompt, options = {}) {
        try {
            const response = await this.client.completions.create({
                model: options.model || 'gpt-3.5-turbo-instruct',
                prompt,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].text
        } catch (error) {
            throw new APIError(error.message, 'openai', error.status)
        }
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.embeddings.create({
                model: options.model || 'text-embedding-3-small',
                input: text,
                ...options
            })
            return response.data[0].embedding
        } catch (error) {
            throw new APIError(error.message, 'openai', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.completions.create({
                model: options.model || 'gpt-4-turbo-preview',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new APIError(error.message, 'openai', error.status)
        }
    }

}

================
File: src/providers/Perplexity.js
================
import OpenAI from 'openai'
import { APIClient, APIError } from '../common/APIClient.js'

export class Perplexity extends APIClient {
constructor(config = {}) {
super(config)
const apiKey = config.apiKey || process.env.PERPLEXITY_API_KEY

        if (!apiKey) {
            throw new Error('Perplexity API key is required. Provide it in constructor or set PERPLEXITY_API_KEY environment variable.')
        }

        this.client = new OpenAI({
            apiKey,
            baseURL: 'https://api.perplexity.ai',
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.completions.create({
                model: options.model || 'pplx-7b-chat',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new APIError(error.message, 'perplexity', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        throw new APIError('Embeddings not supported by Perplexity', 'perplexity', 'UNSUPPORTED_OPERATION')
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.completions.create({
                model: options.model || 'pplx-7b-chat',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new APIError(error.message, 'perplexity', error.status)
        }
    }

}

================
File: tests/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter'

class CustomReporter {
constructor() {
this.specReporter = new SpecReporter({
spec: {
displayPending: true // Display pending (not fully implemented) specs
}
})
}

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments)
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments)
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments)
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments)
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments)
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments)
    }

}

export default CustomReporter;

/\*
import { SpecReporter } from 'jasmine-spec-reporter';

jasmine.getEnv().clearReporters(); // Clear default console reporter
jasmine.getEnv().addReporter(new SpecReporter({
spec: {
displayPending: true // Display pending (not fully implemented) specs
}
}));
\*/

================
File: tests/integration/mcp-integration.spec.js
================
// spec/integration/mcp-integration.spec.js
import { expect } from 'chai'
import { createAPIClient } from '../../src/common/ClientFactory.js'
import fs from 'fs/promises'
import path from 'path'

describe('MCP Integration Tests', () => {
let client
let testDir

    before(async () => {
        // Create test resources directory
        testDir = path.join(process.cwd(), 'test-resources')
        await fs.mkdir(testDir, { recursive: true })
        await fs.writeFile(
            path.join(testDir, 'test.txt'),
            'This is a test document for MCP integration.'
        )

        client = await createAPIClient('openai', {
            apiKey: process.env.OPENAI_API_KEY,
            mcp: {
                resources: {
                    'test-doc': {
                        uri: `file://${path.join(testDir, 'test.txt')}`,
                        mimeType: 'text/plain'
                    }
                },
                tools: {
                    'file-reader': {
                        name: 'File Reader',
                        description: 'Reads file content',
                        execute: async (args) => {
                            const content = await fs.readFile(args.path, 'utf-8')
                            return content
                        }
                    }
                },
                prompts: {
                    'summarize': {
                        name: 'Summarize',
                        template: (ctx) => `Summarize the following text:\n\n${ctx.text}`
                    }
                }
            }
        })
    })

    after(async () => {
        await fs.rm(testDir, { recursive: true })
    })

    it('should read and process real file resources', async () => {
        const resource = await client.getResource('test-doc')
        expect(resource.uri).to.include('test.txt')

        const content = await client.executeTool('file-reader', {
            path: resource.uri.replace('file://', '')
        })
        expect(content).to.include('test document')
    })

    it('should use MCP-enhanced chat completion', async () => {
        const resource = await client.getResource('test-doc')
        const content = await client.executeTool('file-reader', {
            path: resource.uri.replace('file://', '')
        })

        const prompt = await client.renderPrompt('summarize', { text: content })
        const summary = await client.chat([{
            role: 'user',
            content: prompt
        }])

        expect(summary).to.be.a('string')
        expect(summary.length).to.be.greaterThan(0)
    })

})

================
File: tests/unit/base-client.spec.js
================
// spec/base-client.spec.js
import { expect } from 'chai';
import { APIClient, APIError } from '../src/base-client.js';

describe('APIClient', () => {
class TestClient extends APIClient {}

    it('should prevent instantiation of abstract class', () => {
        expect(() => new APIClient()).to.throw('Cannot instantiate abstract class');
    });

    it('should allow instantiation of concrete implementations', () => {
        expect(() => new TestClient()).not.to.throw();
    });

    it('should require implementation of abstract methods', async () => {
        const client = new TestClient();
        await expect(client.chat([])).to.be.rejectedWith('Method chat() must be implemented');
        await expect(client.complete('')).to.be.rejectedWith('Method complete() must be implemented');
        await expect(client.embedding('')).to.be.rejectedWith('Method embedding() must be implemented');
        await expect(client.stream([], () => {})).to.be.rejectedWith('Method stream() must be implemented');
    });

});

describe('APIError', () => {
it('should create error with provider and code', () => {
const error = new APIError('Test error', 'test-provider', 'ERROR_CODE');
expect(error.message).to.equal('Test error');
expect(error.provider).to.equal('test-provider');
expect(error.code).to.equal('ERROR_CODE');
});
});

================
File: tests/unit/claude-2.spec.js
================
import { expect } from 'chai';
import { Claude } from '../../src/providers/Claude.js';

describe('Claude Client', () => {
describe('Unit Tests', () => {
let client;
let mockResponse;

        beforeEach(() => {
            client = new Claude({ apiKey: 'test-key' });
            mockResponse = {
                content: [{ text: 'test response' }],
                embeddings: [[0.1, 0.2, 0.3]]
            };

            // Mock Anthropic client methods
            client.client = {
                messages: {
                    create: async () => mockResponse
                },
                embeddings: {
                    create: async () => ({ embeddings: [[0.1, 0.2, 0.3]] })
                }
            };
        });

        it('should require API key', () => {
            expect(() => new Claude()).to.throw(/API key is required/);
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([
                { role: 'user', content: 'test' }
            ]);
            expect(response).to.equal('test response');
        });

        it('should handle direct completion', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });

        it('should handle streaming', async () => {
            const chunks = [];
            const mockStream = {
                async *[Symbol.asyncIterator]() {
                    yield { delta: { text: 'Hello' } };
                    yield { delta: { text: ' World' } };
                }
            };

            client.client.messages.create = async () => mockStream;

            await client.stream(
                [{ role: 'user', content: 'test' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks).to.deep.equal(['Hello', ' World']);
        });

        it('should handle API errors', async () => {
            client.client.messages.create = async () => {
                throw new Error('API Error');
            };

            await expect(client.chat([]))
                .to.be.rejectedWith('API Error');
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.CLAUDE_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Claude integration tests - no API key');
                return;
            }
            client = new Claude({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is 2+2?' }
            ]);

            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('4');
        });

        it('should generate embeddings', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text');
            expect(embedding).to.be.an('array');
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Count to 3' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.include('1');
            expect(fullResponse).to.include('2');
            expect(fullResponse).to.include('3');
        });

        it('should handle model-specific parameters', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'Hello' }
            ], {
                model: 'claude-3-sonnet-20240229',
                temperature: 0.5,
                maxTokens: 100
            });

            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(0);
        });
    });

});

================
File: tests/unit/claude.spec.js
================
// spec/providers/claude.spec.js
import { expect } from 'chai';
import { Claude } from '../../src/providers/claude.js';

describe('Claude Client', () => {
describe('Unit Tests', () => {
let client;
let mockResponse;

        beforeEach(() => {
            client = new Claude({ apiKey: 'test-key' });
            mockResponse = {
                data: {
                    choices: [{
                        message: { content: 'test response' }
                    }],
                    data: [{ embedding: [0.1, 0.2, 0.3] }]
                }
            };

            client.client = {
                createCompletion: async () => mockResponse,
                createEmbedding: async () => mockResponse,
                createCompletionStream: async () => ({
                    async *[Symbol.asyncIterator]() {
                        yield { choices: [{ delta: { content: 'Hello' } }] };
                    }
                })
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.CLAUDE_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Claude integration tests - no API key');
                return;
            }
            client = new Claude({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is 2+2?' }
            ]);

            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('4');
        });

        it('should generate embeddings', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text');
            expect(embedding).to.be.an('array');
            expect(embedding).to.have.lengthOf(1536);
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Count to 3' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.include('1');
            expect(fullResponse).to.include('2');
            expect(fullResponse).to.include('3');
        });
    });

});

================
File: tests/unit/factory-mcp.spec.js
================
// spec/mcp/factory-mcp.spec.js
import { expect } from 'chai';
import { createAPIClient } from '../../src/factory.js';

describe('MCP Factory Integration', () => {
it('should create MCP-enabled client', async () => {
const client = await createAPIClient('openai', {
apiKey: 'test-key',
mcp: {
resources: {
'test': {
uri: 'mcp:test',
mimeType: 'text/plain'
}
}
}
});

        expect(client.getResource).to.be.a('function');
        expect(client.executeTool).to.be.a('function');
        expect(client.renderPrompt).to.be.a('function');

        const resource = await client.getResource('test');
        expect(resource.uri).to.equal('mcp:test');
    });

    it('should handle MCP tools with provider methods', async () => {
        const mockTool = {
            name: 'Test Tool',
            execute: async () => 'tool result'
        };

        const client = await createAPIClient('openai', {
            apiKey: 'test-key',
            mcp: {
                tools: {
                    'test': mockTool
                }
            }
        });

        client.chat = async () => 'chat result';

        // Should have both MCP and provider methods
        const toolResult = await client.executeTool('test');
        const chatResult = await client.chat([]);

        expect(toolResult).to.equal('tool result');
        expect(chatResult).to.equal('chat result');
    });

    it('should initialize all provided MCP features', async () => {
        const config = {
            apiKey: 'test-key',
            mcp: {
                resources: {
                    'res1': { uri: 'mcp:res1' }
                },
                tools: {
                    'tool1': {
                        name: 'Tool 1',
                        execute: async () => 'result'
                    }
                },
                prompts: {
                    'prompt1': {
                        name: 'Prompt 1',
                        template: (ctx) => `Test ${ctx.value}`
                    }
                }
            }
        };

        const client = await createAPIClient('openai', config);
        const description = client.describe();

        expect(description.resources).to.have.lengthOf(1);
        expect(description.tools).to.have.lengthOf(1);
        expect(description.prompts).to.have.lengthOf(1);

        const promptResult = await client.renderPrompt('prompt1', { value: 'works' });
        expect(promptResult).to.equal('Test works');
    });

    it('should create normal client when MCP not requested', async () => {
        const client = await createAPIClient('openai', {
            apiKey: 'test-key'
        });

        expect(client.getResource).to.be.undefined;
        expect(client.describe).to.be.undefined;
    });

});

================
File: tests/unit/factory.spec.js
================
// spec/factory.spec.js
import { expect } from 'chai';
import { createAPIClient } from '../src/factory.js';
import { OpenAIClient } from '../src/providers/openai.js';
import { Claude } from '../src/providers/claude.js';

describe('APIClient Factory', () => {
it('should create OpenAI client', () => {
const client = createAPIClient('openai', { apiKey: 'test' });
expect(client).to.be.instanceOf(OpenAIClient);
});

    it('should create Claude client', () => {
        const client = createAPIClient('claude', { apiKey: 'test' });
        expect(client).to.be.instanceOf(Claude);
    });

    it('should throw error for unknown provider', () => {
        expect(() => createAPIClient('unknown')).to.throw('Unknown AI provider: unknown');
    });

    it('should be case insensitive', () => {
        const client = createAPIClient('OPENAI', { apiKey: 'test' });
        expect(client).to.be.instanceOf(OpenAIClient);
    });

});

================
File: tests/unit/groq.spec.js
================
// spec/providers/groq.spec.js
import { expect } from 'chai';
import { Groq } from '../../src/providers/groq.js';

describe('Groq Client', () => {
describe('Unit Tests', () => {
let client;
let mockResponse;

        beforeEach(() => {
            client = new Groq({ apiKey: 'test-key' });
            mockResponse = {
                choices: [{
                    message: { content: 'test response' },
                    delta: { content: 'test chunk' }
                }]
            };

            client.client = {
                chat: {
                    completions: {
                        create: async () => mockResponse
                    }
                }
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should throw for embeddings', async () => {
            await expect(client.embedding('test')).to.be.rejectedWith(/not supported/);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.GROQ_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Groq integration tests - no API key');
                return;
            }
            client = new Groq({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.GROQ_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is quantum computing?' }
            ]);

            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(50);
        });

        it('should handle streaming', async function() {
            if (!process.env.GROQ_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Explain recursion briefly' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.include('recurs');
        });
    });

});

================
File: tests/unit/huggingface.spec.js
================
// spec/providers/huggingface.spec.js
import { expect } from 'chai';
import { HuggingFace } from '../../src/providers/huggingface.js';

describe('HuggingFace Client', () => {
describe('Unit Tests', () => {
let client;
let mockResponse;

        beforeEach(() => {
            client = new HuggingFace({ apiKey: 'test-key' });
            mockResponse = {
                generated_text: 'test response',
                embeddings: [[0.1, 0.2, 0.3]]
            };

            client.client = {
                textGeneration: async () => mockResponse,
                featureExtraction: async () => [[0.1, 0.2, 0.3]]
            };
        });

        it('should handle text generation', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });

        it('should throw for streaming', async () => {
            await expect(client.stream([])).to.be.rejectedWith(/not supported/);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.HUGGINGFACE_API_KEY;
            if (!apiKey) {
                console.warn('Skipping HuggingFace integration tests - no API key');
                return;
            }
            client = new HuggingFace({ apiKey });
        });

        it('should perform text completion', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.complete('The capital of France is');
            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('paris');
        });

        it('should generate embeddings', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text', {
                model: 'sentence-transformers/all-MiniLM-L6-v2'
            });

            expect(embedding).to.be.an('array');
            expect(embedding.length).to.equal(384); // MiniLM dimension
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle chat-like interactions', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is machine learning?' }
            ]);

            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(20);
        });

        it('should handle model-specific parameters', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.complete('Once upon a time', {
                model: 'gpt2',
                parameters: {
                    do_sample: true,
                    max_new_tokens: 50,
                    temperature: 0.7
                }
            });

            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(10);
        });
    });

});

================
File: tests/unit/mcp-client.spec.js
================
// spec/mcp/mcp-client.spec.js
import { expect } from 'chai';
import { MCPClient } from '../../src/mcp/mcp-client.js';
import { APIError } from '../../src/base-client.js';

describe('MCP Client', () => {
let client;

    beforeEach(() => {
        client = new MCPClient();
    });

    describe('Resource Management', () => {
        it('should register and retrieve resources', async () => {
            await client.registerResource('test', {
                uri: 'mcp:test',
                mimeType: 'text/plain'
            });

            const resource = await client.getResource('test');
            expect(resource.uri).to.equal('mcp:test');
            expect(resource.mimeType).to.equal('text/plain');
        });

        it('should generate default URI if not provided', async () => {
            await client.registerResource('test', {});
            const resource = await client.getResource('test');
            expect(resource.uri).to.equal('mcp:resource:test');
        });

        it('should throw on missing resource', async () => {
            await expect(client.getResource('nonexistent'))
                .to.be.rejectedWith(APIError)
                .and.have.property('code', 'RESOURCE_NOT_FOUND');
        });
    });

    describe('Tool Management', () => {
        it('should register and execute tools', async () => {
            const tool = {
                name: 'Test Tool',
                description: 'A test tool',
                execute: async (args) => args.value * 2
            };

            await client.registerTool('multiply', tool);
            const result = await client.executeTool('multiply', { value: 5 });
            expect(result).to.equal(10);
        });

        it('should throw on non-executable tool', async () => {
            await client.registerTool('broken', { name: 'Broken Tool' });
            await expect(client.executeTool('broken'))
                .to.be.rejectedWith(APIError)
                .and.have.property('code', 'TOOL_NOT_EXECUTABLE');
        });
    });

    describe('Prompt Management', () => {
        it('should register and render prompts', async () => {
            const prompt = {
                name: 'Test Prompt',
                template: (ctx) => `Hello ${ctx.name}!`
            };

            await client.registerPrompt('greeting', prompt);
            const result = await client.renderPrompt('greeting', { name: 'World' });
            expect(result).to.equal('Hello World!');
        });

        it('should throw on invalid prompt', async () => {
            await client.registerPrompt('invalid', { name: 'Invalid Prompt' });
            await expect(client.renderPrompt('invalid'))
                .to.be.rejectedWith(APIError)
                .and.have.property('code', 'INVALID_PROMPT');
        });
    });

    describe('MCP Description', () => {
        it('should describe all registered capabilities', async () => {
            await client.registerResource('res1', { mimeType: 'text/plain' });
            await client.registerTool('tool1', { name: 'Tool 1', description: 'Test tool' });
            await client.registerPrompt('prompt1', { name: 'Prompt 1', description: 'Test prompt' });

            const description = client.describe();

            expect(description.resources).to.have.lengthOf(1);
            expect(description.tools).to.have.lengthOf(1);
            expect(description.prompts).to.have.lengthOf(1);

            expect(description.resources[0].id).to.equal('res1');
            expect(description.tools[0].name).to.equal('Tool 1');
            expect(description.prompts[0].name).to.equal('Prompt 1');
        });
    });

});

================
File: tests/unit/mcp-persistence.spec.js
================
// spec/integration/mcp-persistence.spec.js
import { expect } from 'chai';
import { createAPIClient } from '../../src/factory.js';
import fs from 'fs/promises';
import path from 'path';

describe('MCP State Persistence', () => {
const stateFile = path.join(process.cwd(), 'mcp-state.json');
let client;

    const persistState = async (state) => {
        await fs.writeFile(stateFile, JSON.stringify(state, null, 2));
    };

    const loadState = async () => {
        const data = await fs.readFile(stateFile, 'utf-8');
        return JSON.parse(data);
    };

    beforeEach(async () => {
        // Initial state
        const initialState = {
            resources: {
                'persisted-doc': {
                    uri: 'mcp:test',
                    mimeType: 'text/plain',
                    content: 'Persisted content'
                }
            },
            tools: {
                'persisted-tool': {
                    name: 'Persisted Tool',
                    description: 'A tool that persists',
                    results: []
                }
            }
        };
        await persistState(initialState);

        client = await createAPIClient('openai', {
            apiKey: process.env.OPENAI_API_KEY,
            mcp: {
                stateFile,
                onStateChange: async (newState) => {
                    await persistState(newState);
                }
            }
        });
    });

    afterEach(async () => {
        try {
            await fs.unlink(stateFile);
        } catch (err) {
            // Ignore if file doesn't exist
        }
    });

    it('should load persisted resources', async () => {
        const resource = await client.getResource('persisted-doc');
        expect(resource.uri).to.equal('mcp:test');
        expect(resource.content).to.equal('Persisted content');
    });

    it('should persist new resources', async () => {
        await client.registerResource('new-doc', {
            uri: 'mcp:new',
            content: 'New content'
        });

        const state = await loadState();
        expect(state.resources['new-doc']).to.exist;
        expect(state.resources['new-doc'].content).to.equal('New content');
    });

    it('should track tool execution history', async () => {
        const toolState = {
            name: 'History Tool',
            description: 'Tracks execution history',
            execute: async (args) => {
                const state = await loadState();
                const tool = state.tools['history-tool'];
                tool.results.push(args);
                await persistState(state);
                return args.value * 2;
            },
            results: []
        };

        await client.registerTool('history-tool', toolState);
        await client.executeTool('history-tool', { value: 5 });

        const state = await loadState();
        expect(state.tools['history-tool'].results).to.have.lengthOf(1);
        expect(state.tools['history-tool'].results[0].value).to.equal(5);
    });

    it('should restore complete MCP state on restart', async () => {
        // Register new items
        await client.registerResource('test1', { uri: 'mcp:test1' });
        await client.registerTool('tool1', { name: 'Tool 1' });
        await client.registerPrompt('prompt1', { name: 'Prompt 1' });

        // Create new client instance
        const newClient = await createAPIClient('openai', {
            apiKey: process.env.OPENAI_API_KEY,
            mcp: { stateFile }
        });

        const description = newClient.describe();
        expect(description.resources).to.have.lengthOf(2); // includes persisted-doc
        expect(description.tools).to.have.lengthOf(2);     // includes persisted-tool
        expect(description.prompts).to.have.lengthOf(1);
    });

});

================
File: tests/unit/mistral.spec.js
================
// spec/providers/mistral.spec.js
import { expect } from 'chai';
import { MistralAIClient } from '../../src/providers/mistral.js';

describe('Mistral Client', () => {
describe('Unit Tests', () => {
let client;
let mockResponse;

        beforeEach(() => {
            client = new MistralAIClient({ apiKey: 'test-key' });
            mockResponse = {
                choices: [{
                    message: { content: 'test response' }
                }],
                data: [{ embedding: [0.1, 0.2, 0.3] }]
            };

            client.client = {
                chat: {
                    create: async () => mockResponse,
                    stream: async () => ({
                        async *[Symbol.asyncIterator]() {
                            yield { choices: [{ delta: { content: 'Hello' } }] };
                        }
                    })
                },
                embeddings: {
                    create: async () => mockResponse
                }
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.MISTRAL_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Mistral integration tests - no API key');
                return;
            }
            client = new MistralAIClient({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is the capital of France?' }
            ]);

            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('paris');
        });

        it('should generate embeddings', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text');
            expect(embedding).to.be.an('array');
            expect(embedding).to.have.length.greaterThan(0);
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Tell me a short joke' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.be.a('string').and.to.have.length.greaterThan(10);
        });

        it('should handle errors gracefully', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const badClient = new MistralAIClient({ apiKey: 'invalid-key' });
            await expect(badClient.chat([
                { role: 'user', content: 'test' }
            ])).to.be.rejected;
        });
    });

});

================
File: tests/unit/ollama.spec.js
================
// spec/providers/ollama.spec.js
import { expect } from 'chai';
import { Ollama } from '../../src/providers/ollama.js';

describe('Ollama Client', () => {
describe('Unit Tests', () => {
let client;
let mockOllama;

        beforeEach(() => {
            client = new Ollama({ baseUrl: 'http://localhost:11434' });
            mockOllama = {
                chat: async () => ({ message: { content: 'test response' } }),
                generate: async () => ({ response: 'test completion' }),
                embeddings: async () => ({ embedding: [0.1, 0.2, 0.3] })
            };
            global.ollama = mockOllama;
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle direct completion', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test completion');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(async () => {
            // Check if Ollama is running
            try {
                const response = await fetch('http://localhost:11434/api/version');
                if (!response.ok) {
                    console.warn('Skipping Ollama integration tests - service not running');
                    return;
                }
            } catch (e) {
                console.warn('Skipping Ollama integration tests - service not available');
                return;
            }
            client = new Ollama({});
        });

        it('should perform chat completion', async function() {
            if (!client) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is 2+2?' }
            ], { model: 'llama2' });

            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('4');
        });

        it('should generate embeddings', async function() {
            if (!client) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text', {
                model: 'nomic-embed-text'
            });

            expect(embedding).to.be.an('array');
            expect(embedding).to.have.lengthOf(768); // nomic-embed-text dimension
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!client) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Count to 3' }],
                chunk => chunks.push(chunk),
                { model: 'llama2' }
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.match(/[123]/);
        });

        it('should handle model not found errors', async function() {
            if (!client) {
                this.skip();
                return;
            }

            await expect(client.chat([
                { role: 'user', content: 'test' }
            ], { model: 'nonexistent-model' })).to.be.rejectedWith(/model.*not found/i);
        });
    });

});

================
File: tests/unit/openai.spec.js
================
// spec/providers/openai.spec.js
import { expect } from 'chai';
import { OpenAIClient } from '../../src/providers/openai.js';

describe('OpenAI Client', () => {
let client;
let mockResponse;

    beforeEach(() => {
        client = new OpenAIClient({ apiKey: 'test-key' });
        mockResponse = {
            choices: [{
                message: { content: 'test response' },
                text: 'test completion'
            }],
            data: [{ embedding: [0.1, 0.2, 0.3] }]
        };

        // Mock OpenAI client methods
        client.client = {
            chat: {
                completions: {
                    create: async () => mockResponse
                }
            },
            completions: {
                create: async () => mockResponse
            },
            embeddings: {
                create: async () => mockResponse
            }
        };
    });

    describe('chat', () => {
        it('should return chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle errors', async () => {
            client.client.chat.completions.create = async () => {
                throw new Error('API Error');
            };
            await expect(client.chat([])).to.be.rejectedWith('API Error');
        });
    });

    describe('complete', () => {
        it('should return completion', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test completion');
        });
    });

    describe('embedding', () => {
        it('should return embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('stream', () => {
        it('should handle streaming responses', async () => {
            const chunks = [];
            const mockStream = {
                async *[Symbol.asyncIterator]() {
                    yield { choices: [{ delta: { content: 'Hello' } }] };
                    yield { choices: [{ delta: { content: ' World' } }] };
                }
            };

            client.client.chat.completions.create = async () => mockStream;

            await client.stream(
                [{ role: 'user', content: 'test' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks).to.deep.equal(['Hello', ' World']);
        });
    });

});

================
File: tests/unit/perplexity.spec.js
================
// spec/providers/perplexity.spec.js
import { expect } from 'chai';
import { Perplexity } from '../../src/providers/perplexity.js';

describe('Perplexity Client', () => {
describe('Unit Tests', () => {
let client;
let mockResponse;

        beforeEach(() => {
            client = new Perplexity({ apiKey: 'test-key' });
            mockResponse = {
                choices: [{
                    message: { content: 'test response' },
                    delta: { content: 'test chunk' }
                }]
            };

            client.client = {
                chat: {
                    completions: {
                        create: async () => mockResponse
                    }
                }
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should throw for embeddings', async () => {
            await expect(client.embedding('test')).to.be.rejectedWith(/not supported/);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.PERPLEXITY_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Perplexity integration tests - no API key');
                return;
            }
            client = new Perplexity({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.PERPLEXITY_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is the largest moon in our solar system?' }
            ]);

            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('ganymede');
        });

        it('should handle streaming', async function() {
            if (!process.env.PERPLEXITY_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Name three planets' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.match(/mercury|venus|mars|jupiter|saturn|uranus|neptune/i);
        });

        it('should handle rate limits', async function() {
            if (!process.env.PERPLEXITY_API_KEY) {
                this.skip();
                return;
            }

            const promises = Array(10).fill().map(() =>
                client.chat([{ role: 'user', content: 'test' }])
            );

            await expect(Promise.all(promises))
                .to.be.rejectedWith(/rate limit/i);
        });
    });

});

================
File: .gitignore
================
.env

# Logs

logs
_.log
npm-debug.log_
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)

report.[0-9]_.[0-9]_.[0-9]_.[0-9]_.json

# Runtime data

pids
_.pid
_.seed
\*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover

lib-cov

# Coverage directory used by tools like istanbul

coverage
\*.lcov

# nyc test coverage

.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)

.grunt

# Bower dependency directory (https://bower.io/)

bower_components

# node-waf configuration

.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)

build/Release

# Dependency directories

node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)

web_modules/

# TypeScript cache

\*.tsbuildinfo

# Optional npm cache directory

.npm

# Optional eslint cache

.eslintcache

# Optional stylelint cache

.stylelintcache

# Microbundle cache

.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history

.node_repl_history

# Output of 'npm pack'

\*.tgz

# Yarn Integrity file

.yarn-integrity

# dotenv environment variable files

.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)

.cache
.parcel-cache

# Next.js build output

.next
out

# Nuxt.js build / generate output

.nuxt
dist

# Gatsby files

.cache/

# Comment in the public line in if your project uses Gatsby and not Next.js

# https://nextjs.org/blog/next-9-1#public-directory-support

# public

# vuepress build output

.vuepress/dist

# vuepress v2.x temp and cache directory

.temp
.cache

# Docusaurus cache and generated files

.docusaurus

# Serverless directories

.serverless/

# FuseBox cache

.fusebox/

# DynamoDB Local files

.dynamodb/

# TernJS port file

.tern-port

# Stores VSCode versions used for testing VSCode extensions

.vscode-test

# yarn v2

.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.\*

================
File: clients.d.ts
================
// clients.d.ts
import { AIConfig, Message, MCPResource, MCPTool, MCPPrompt, MCPState } from './index';

export abstract class APIClient {
constructor(config?: AIConfig);
abstract chat(messages: Message[], options?: Record<string, any>): Promise<string>;
abstract complete(prompt: string, options?: Record<string, any>): Promise<string>;
abstract embedding(text: string | string[], options?: Record<string, any>): Promise<number[]>;
abstract stream(messages: Message[], callback: (chunk: string) => void, options?: Record<string, any>): Promise<void>;
}

export class MCPClient extends APIClient {
registerResource(id: string, resource: MCPResource): Promise<void>;
registerTool(id: string, tool: MCPTool): Promise<void>;
registerPrompt(id: string, prompt: MCPPrompt): Promise<void>;
getResource(id: string): Promise<MCPResource>;
executeTool(id: string, args: Record<string, any>): Promise<any>;
renderPrompt(id: string, context: Record<string, any>): Promise<string>;
describe(): MCPState;
}

export function createAPIClient(
provider: string,
config?: AIConfig
): Promise<APIClient & Partial<MCPClient>>;

// Provider-specific types
export interface OpenAIOptions {
model?: string;
temperature?: number;
maxTokens?: number;
[key: string]: any;
}

export interface MistralOptions {
model?: string;
maxTokens?: number;
temperature?: number;
[key: string]: any;
}

export interface OllamaOptions {
model?: string;
parameters?: {
temperature?: number;
num_ctx?: number;
[key: string]: any;
};
}

================
File: env-example.txt
================

# .env.example

OPENAI_API_KEY=your_openai_key
CLAUDE_API_KEY=your_claude_key
MISTRAL_API_KEY=your_mistral_key
GROQ_API_KEY=your_groq_key
PERPLEXITY_API_KEY=your_perplexity_key
HUGGINGFACE_API_KEY=your_huggingface_key

# Ollama settings

OLLAMA_HOST=http://localhost:11434

# Test settings

TEST_TIMEOUT=10000

================
File: github-workflow (1).txt
================
name: Test Suite

on:
push:
branches: [ main ]
pull_request:
branches: [ main ]

jobs:
test:
runs-on: ubuntu-latest
services:
ollama:
image: ollama/ollama:latest
ports: - 11434:11434

    strategy:
      matrix:
        node-version: [18.x, 20.x]

    steps:
    - uses: actions/checkout@v4

    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Setup test environment
      run: |
        cp .env.example .env.test
        echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> .env.test
        echo "CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }}" >> .env.test
        echo "MISTRAL_API_KEY=${{ secrets.MISTRAL_API_KEY }}" >> .env.test
        echo "GROQ_API_KEY=${{ secrets.GROQ_API_KEY }}" >> .env.test
        echo "PERPLEXITY_API_KEY=${{ secrets.PERPLEXITY_API_KEY }}" >> .env.test
        echo "HUGGINGFACE_API_KEY=${{ secrets.HUGGINGFACE_API_KEY }}" >> .env.test

    - name: Pull Ollama models
      run: |
        curl http://localhost:11434/api/pull -d '{"name":"llama2"}'
        curl http://localhost:11434/api/pull -d '{"name":"nomic-embed-text"}'

    - name: Run unit tests
      run: npm test

    - name: Run integration tests
      if: github.event_name == 'push'
      run: NODE_ENV=test npm test -- spec/integration

    - name: Upload coverage
      uses: codecov/codecov-action@v4

================
File: github-workflow.txt
================
name: Test Suite

on:
push:
branches: [ main ]
pull_request:
branches: [ main ]

jobs:
test:
runs-on: ubuntu-latest
services:
ollama:
image: ollama/ollama:latest
ports: - 11434:11434

    strategy:
      matrix:
        node-version: [18.x, 20.x]

    steps:
    - uses: actions/checkout@v4

    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Setup test environment
      run: |
        cp .env.example .env.test
        echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> .env.test
        echo "CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }}" >> .env.test
        echo "MISTRAL_API_KEY=${{ secrets.MISTRAL_API_KEY }}" >> .env.test
        echo "GROQ_API_KEY=${{ secrets.GROQ_API_KEY }}" >> .env.test
        echo "PERPLEXITY_API_KEY=${{ secrets.PERPLEXITY_API_KEY }}" >> .env.test
        echo "HUGGINGFACE_API_KEY=${{ secrets.HUGGINGFACE_API_KEY }}" >> .env.test

    - name: Pull Ollama models
      run: |
        curl http://localhost:11434/api/pull -d '{"name":"llama2"}'
        curl http://localhost:11434/api/pull -d '{"name":"nomic-embed-text"}'

    - name: Run unit tests
      run: npm test

    - name: Run integration tests
      if: github.event_name == 'push'
      run: NODE_ENV=test npm test -- spec/integration

    - name: Upload coverage
      uses: codecov/codecov-action@v4

================
File: index.d.ts.ts
================
// index.d.ts
export interface AIConfig {
apiKey?: string;
clientOptions?: Record<string, any>;
mcp?: MCPConfig;
}

export interface MCPConfig {
resources?: Record<string, MCPResource>;
tools?: Record<string, MCPTool>;
prompts?: Record<string, MCPPrompt>;
stateFile?: string;
onStateChange?: (state: MCPState) => Promise<void>;
}

export interface MCPResource {
uri?: string;
mimeType?: string;
[key: string]: any;
}

export interface MCPTool {
name: string;
description?: string;
execute: (args: Record<string, any>) => Promise<any>;
}

export interface MCPPrompt {
name: string;
description?: string;
template: (context: Record<string, any>) => string;
}

export interface MCPState {
resources: Record<string, MCPResource>;
tools: Record<string, MCPTool>;
prompts: Record<string, MCPPrompt>;
}

export interface Message {
role: 'user' | 'assistant' | 'system';
content: string;
}

export class APIError extends Error {
constructor(message: string, provider: string, code: string);
provider: string;
code: string;
}

================
File: jasmine.json
================
{
"spec_dir": "tests",
"spec_files": [
"**/\*[sS]pec.?(m)js"
],
"helpers": [
"helpers/**/\*.?(m)js"
],
"env": {
"stopSpecOnExpectationFailure": false,
"random": true
}
}

================
File: package.json
================
{
"name": "hyperdata-clients",
"version": "1.0.0",
"type": "module",
"description": "Unified client library for multiple AI providers",
"main": "src/factory.js",
"scripts": {
"coverage": "NODE_ENV=test c8 npm test",
"test": "jasmine --config=jasmine.json --reporter=tests/helpers/reporter.js",
"docs": "jsdoc -c jsdoc.json",
"rp": "repomix -c repomix.config.json . "
},
"dependencies": {
"@anthropic-ai/sdk": "^0.17.1",
"@huggingface/inference": "^2.6.4",
"@mistralai/mistralai": "^0.0.10",
"dotenv": "^16.4.1",
"groq-sdk": "^0.3.0",
"ollama": "^0.4.4",
"openai": "^4.28.0"
},
"devDependencies": {
"c8": "^9.1.0",
"chai": "^5.0.3",
"jasmine": "^5.5.0",
"jasmine-spec-reporter": "^7.0.0",
"jsdoc": "^4.0.4"
},
"engines": {
"node": ">=18.0.0"
}
}

================
File: readme.md
================

# AI Client Library

Unified client library for interacting with multiple AI providers including OpenAI, Claude, Mistral, Groq, Perplexity, HuggingFace and Ollama.

## Installation

```bash
npm install ai-client-library
```

## Usage

```javascript
import { createAPIClient } from "ai-client-library"

const client = createAPIClient("openai", {
  apiKey: "your-api-key",
})

// Chat completion
const response = await client.chat([{ role: "user", content: "Hello!" }])

// Stream response
await client.stream([{ role: "user", content: "Tell me a story" }], (chunk) =>
  console.log(chunk)
)
```

## Supported Providers

- OpenAI
- Claude (Anthropic)
- Ollama
- Mistral
- Groq
- Perplexity
- HuggingFace

## Development

```bash
npm install
npm test
```

## License

MIT

================
File: README.md
================

# clients

some API clients

================
File: repomix.config.json
================
{
"output": {
"filePath": "./repomix-clients.md",
"headerText": "clients repo",
"removeComments": false
},
"include": [
"**/*"
],
"ignore": {
"useDefaultPatterns": false,
"customPatterns": [
"docs",
"data",
"docs/jsdoc",
".nyc_output",
".env",
"**/_*",
"node_modules",
"*.log",
"**/*repomix*.txt",
"**/*.html",
"**/data/*",
"**/*copy.js",
"**/conversations.json"
]
}
}

================
File: test-env.txt
================

# .env.test

OPENAI_API_KEY=test_key
CLAUDE_API_KEY=test_key
MISTRAL_API_KEY=test_key
GROQ_API_KEY=test_key
PERPLEXITY_API_KEY=test_key
HUGGINGFACE_API_KEY=test_key
OLLAMA_HOST=http://localhost:11434
TEST_TIMEOUT=1000

================
File: types-package.json
================
// package.json
{
"name": "@types/ai-client-library",
"version": "1.0.0",
"types": "index.d.ts",
"dependencies": {
"@types/node": "^20.0.0"
}
}

================================================================
End of Codebase
================================================================
