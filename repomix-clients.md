This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-25T23:21:22.891Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------
User Provided Header:
-----------------------
clients repo

================================================================
Directory Structure
================================================================
examples/
  basic.js
  mcp.js
src/
  common/
    AIClient.js
    ClientFactory.js
    KeyManager.js
  providers/
    ClaudeClient.js
    GroqClient.js
    HuggingFaceClient.js
    MCPClient.js
    MistralClient.js
    OllamaClient.js
    OpenAIClient.js
    PerplexityClient.js
tests/
  helpers/
    reporter.js
  integration/
    mcp-integration.spec.js
  unit/
    base-client.spec.js
    claude-2.spec.js
    claude.spec.js
    factory-mcp.spec.js
    factory.spec.js
    groq.spec.js
    huggingface.spec.js
    mcp-client.spec.js
    mcp-persistence.spec.js
    mistral.spec.js
    ollama.spec.js
    openai.spec.js
    perplexity.spec.js
.git
.gitignore
clients.d.ts
env-example.txt
github-workflow (1).txt
github-workflow.txt
index.d.ts.ts
jasmine.json
package.json
readme.md
README.md
repomix.config.json
test-env.txt
types-package.json

================================================================
Files
================================================================

================
File: examples/basic.js
================
import { createAIClient } from '../common/ClientFactory.js';

const client = createAIClient('openai', { apiKey: 'your-key' });
const response = await client.chat([
    { role: 'user', content: 'Hello!' } // should be using .env
]);

================
File: examples/mcp.js
================
const client = await createAIClient('openai', {
    mcp: {
        resources: {
            'docs': {
                uri: 'file://docs/',
                mimeType: 'text/markdown'
            }
        },
        tools: {
            'search': {
                name: 'Search Documentation',
                description: 'Search through documentation',
                execute: async (query) => { /* ... */ }
            }
        },
        prompts: {
            'summarize': {
                name: 'Summarize Text',
                template: (context) => `Summarize: ${context.text}`
            }
        }
    }
});

// Use MCP features
const resource = await client.getResource('docs');
const result = await client.executeTool('search', { query: 'api' });
const prompt = await client.renderPrompt('summarize', { text: 'content' });

================
File: src/common/AIClient.js
================
export class AIClient {
    constructor(config = {}) {
        if (this.constructor === AIClient) {
            throw new Error('Cannot instantiate abstract class');
        }
        this.config = config;
    }

    async chat(messages, options = {}) {
        throw new Error('Method chat() must be implemented');
    }

    async complete(prompt, options = {}) {
        throw new Error('Method complete() must be implemented');
    }

    async embedding(text, options = {}) {
        throw new Error('Method embedding() must be implemented');
    }

    async stream(messages, callback, options = {}) {
        throw new Error('Method stream() must be implemented');
    }
}

export class AIError extends Error {
    constructor(message, provider, code) {
        super(message);
        this.name = 'AIError';
        this.provider = provider;
        this.code = code;
    }
}

================
File: src/common/ClientFactory.js
================
import { OpenAIClient } from '../providers/OpenAIClient.js'
import { ClaudeClient } from '../providers/ClaudeClient.js'
import { OllamaClient } from '../providers/OllamaClient.js'
import { MistralClient } from '../providers/MistralClient.js'
import { GroqClient } from '../providers/GroqClient.js'
import { PerplexityClient } from '../providers/PerplexityClient.js'
import { HuggingFaceClient } from '../providers/HuggingFaceClient.js'
import { MCPClient } from '../providers/MCPClient.js'
import { KeyManager } from '../common/KeyManager.js'

const PROVIDERS = {
    openai: OpenAIClient,
    claude: ClaudeClient,
    ollama: OllamaClient,
    mistral: MistralClient,
    groq: GroqClient,
    perplexity: PerplexityClient,
    huggingface: HuggingFaceClient
}

export async function createAIClient(provider, config = {}) {
    const ClientClass = PROVIDERS[provider.toLowerCase()]
    if (!ClientClass) {
        throw new Error(`Unknown AI provider: ${provider}`)
    }

    // Validate and get API key
    const key = KeyManager.getKey(config, provider)

    // Create base client
    const client = new ClientClass({ ...config, apiKey: key })

    // Wrap with MCP if requested
    if (config.mcp) {
        const mcpClient = new MCPClient(config.mcp)

        // Register MCP resources if provided
        if (config.mcp.resources) {
            for (const [id, resource] of Object.entries(config.mcp.resources)) {
                await mcpClient.registerResource(id, resource)
            }
        }

        // Register MCP tools if provided
        if (config.mcp.tools) {
            for (const [id, tool] of Object.entries(config.mcp.tools)) {
                await mcpClient.registerTool(id, tool)
            }
        }

        // Register MCP prompts if provided
        if (config.mcp.prompts) {
            for (const [id, prompt] of Object.entries(config.mcp.prompts)) {
                await mcpClient.registerPrompt(id, prompt)
            }
        }

        // Extend client with MCP capabilities
        return new Proxy(client, {
            get(target, prop) {
                if (prop in mcpClient) {
                    return mcpClient[prop].bind(mcpClient)
                }
                return target[prop]
            }
        })
    }

    return client
}

================
File: src/common/KeyManager.js
================
export class KeyManager {
    static validateKey(key, provider) {
        if (!key) {
            throw new Error(`${provider} API key is required`);
        }

        const patterns = {
            openai: /^sk-[a-zA-Z0-9]{32,}$/,
            claude: /^sk-ant-[a-zA-Z0-9]{32,}$/,
            mistral: /^[a-zA-Z0-9]{32,}$/,
            groq: /^gsk_[a-zA-Z0-9]{32,}$/,
            perplexity: /^pplx-[a-zA-Z0-9]{32,}$/,
            huggingface: /^hf_[a-zA-Z0-9]{32,}$/
        };

        if (patterns[provider] && !patterns[provider].test(key)) {
            throw new Error(`Invalid ${provider} API key format`);
        }
    }

    static getKey(config, provider) {
        const key = config.apiKey || process.env[`${provider.toUpperCase()}_API_KEY`];
        this.validateKey(key, provider);
        return key;
    }

    static rotateKey(config, provider, newKey) {
        this.validateKey(newKey, provider);
        const envVar = `${provider.toUpperCase()}_API_KEY`;
        process.env[envVar] = newKey;
        return newKey;
    }
}

================
File: src/providers/ClaudeClient.js
================
import Anthropic from '@anthropic-ai/sdk'
import { AIClient, AIError } from '../common/AIClient.js'

export class ClaudeClient extends AIClient {
    constructor(config = {}) {
        super(config)
        const apiKey = config.apiKey || process.env.CLAUDE_API_KEY

        if (!apiKey) {
            throw new Error('Claude API key is required. Provide it in constructor or set CLAUDE_API_KEY environment variable.')
        }

        this.client = new Anthropic({
            apiKey,
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.messages.create({
                model: options.model || 'claude-3-opus-20240229',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.content[0].text
        } catch (error) {
            throw new AIError(error.message, 'claude', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.embeddings.create({
                model: options.model || 'claude-3-embedding',
                input: text instanceof Array ? text : [text],
                ...options
            })
            return response.embeddings[0]
        } catch (error) {
            throw new AIError(error.message, 'claude', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.messages.create({
                model: options.model || 'claude-3-opus-20240229',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.delta?.text || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new AIError(error.message, 'claude', error.status)
        }
    }
}

================
File: src/providers/GroqClient.js
================
import { Groq } from 'groq-sdk'
import { AIClient, AIError } from '../common/AIClient.js'

export class GroqClient extends AIClient {
    constructor(config = {}) {
        super(config)
        const apiKey = config.apiKey || process.env.GROQ_API_KEY

        if (!apiKey) {
            throw new Error('Groq API key is required. Provide it in constructor or set GROQ_API_KEY environment variable.')
        }

        this.client = new Groq({
            apiKey,
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.completions.create({
                model: options.model || 'llama3-8b-8192',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new AIError(error.message, 'groq', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        throw new AIError('Embeddings not supported by Groq', 'groq', 'UNSUPPORTED_OPERATION')
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.completions.create({
                model: options.model || 'llama3-8b-8192',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new AIError(error.message, 'groq', error.status)
        }
    }
}

================
File: src/providers/HuggingFaceClient.js
================
import { HfInference } from '@huggingface/inference'
import { AIClient, AIError } from '../common/AIClient.js'

export class HuggingFaceClient extends AIClient {
    constructor(config = {}) {
        super(config)
        const apiKey = config.apiKey || process.env.HUGGINGFACE_API_KEY

        if (!apiKey) {
            throw new Error('HuggingFace API key is required. Provide it in constructor or set HUGGINGFACE_API_KEY environment variable.')
        }

        this.client = new HfInference(apiKey)
    }

    async chat(messages, options = {}) {
        try {
            const input = messages.map(m => `${m.role}: ${m.content}`).join('\n')
            const response = await this.client.textGeneration({
                model: options.model || 'gpt2',
                inputs: input,
                parameters: {
                    max_new_tokens: options.maxTokens || 50,
                    temperature: options.temperature || 0.7,
                    ...options.parameters
                }
            })
            return response.generated_text
        } catch (error) {
            throw new AIError(error.message, 'huggingface', error.status)
        }
    }

    async complete(prompt, options = {}) {
        try {
            const response = await this.client.textGeneration({
                model: options.model || 'gpt2',
                inputs: prompt,
                parameters: {
                    max_new_tokens: options.maxTokens || 50,
                    temperature: options.temperature || 0.7,
                    ...options.parameters
                }
            })
            return response.generated_text
        } catch (error) {
            throw new AIError(error.message, 'huggingface', error.status)
        }
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.featureExtraction({
                model: options.model || 'sentence-transformers/all-MiniLM-L6-v2',
                inputs: text instanceof Array ? text : [text],
                ...options
            })
            return response[0]
        } catch (error) {
            throw new AIError(error.message, 'huggingface', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        throw new AIError('Streaming not supported by HuggingFace Inference API', 'huggingface', 'UNSUPPORTED_OPERATION')
    }
}

================
File: src/providers/MCPClient.js
================
import { AIClient, AIError } from '../common/AIClient.js'

export class MCPClient extends AIClient {
    constructor(config = {}) {
        super(config)
        this.resources = new Map()
        this.tools = new Map()
        this.prompts = new Map()
    }

    async registerResource(id, resource) {
        this.resources.set(id, {
            ...resource,
            uri: resource.uri || `mcp:resource:${id}`,
            mimeType: resource.mimeType || 'text/plain'
        })
    }

    async registerTool(id, tool) {
        this.tools.set(id, {
            ...tool,
            name: tool.name || id,
            description: tool.description || ''
        })
    }

    async registerPrompt(id, prompt) {
        this.prompts.set(id, {
            ...prompt,
            name: prompt.name || id,
            description: prompt.description || ''
        })
    }

    async getResource(id) {
        const resource = this.resources.get(id)
        if (!resource) throw new AIError(`Resource not found: ${id}`, 'mcp', 'RESOURCE_NOT_FOUND')
        return resource
    }

    async executeTool(id, args) {
        const tool = this.tools.get(id)
        if (!tool) throw new AIError(`Tool not found: ${id}`, 'mcp', 'TOOL_NOT_FOUND')
        if (!tool.execute) throw new AIError(`Tool not executable: ${id}`, 'mcp', 'TOOL_NOT_EXECUTABLE')
        return tool.execute(args)
    }

    async renderPrompt(id, context) {
        const prompt = this.prompts.get(id)
        if (!prompt) throw new AIError(`Prompt not found: ${id}`, 'mcp', 'PROMPT_NOT_FOUND')
        if (!prompt.template) throw new AIError(`Invalid prompt template: ${id}`, 'mcp', 'INVALID_PROMPT')
        return prompt.template(context)
    }

    describe() {
        return {
            resources: Array.from(this.resources.entries()).map(([id, r]) => ({
                id,
                uri: r.uri,
                mimeType: r.mimeType
            })),
            tools: Array.from(this.tools.entries()).map(([id, t]) => ({
                id,
                name: t.name,
                description: t.description
            })),
            prompts: Array.from(this.prompts.entries()).map(([id, p]) => ({
                id,
                name: p.name,
                description: p.description
            }))
        }
    }
}

================
File: src/providers/MistralClient.js
================
import MistralClient from '@mistralai/mistralai'
import { AIClient, AIError } from '../common/AIClient.js'

export class MistralClient extends AIClient {
    constructor(config = {}) {
        super(config)
        const apiKey = config.apiKey || process.env.MISTRAL_API_KEY

        if (!apiKey) {
            throw new Error('Mistral API key is required. Provide it in constructor or set MISTRAL_API_KEY environment variable.')
        }

        this.client = new MistralClient({
            apiKey,
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.create({
                model: options.model || 'mistral-tiny',
                messages,
                maxTokens: options.maxTokens,
                temperature: options.temperature || 0.7,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new AIError(error.message, 'mistral', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.embeddings.create({
                model: options.model || 'mistral-embed',
                input: text instanceof Array ? text : [text],
                ...options
            })
            return response.data[0].embedding
        } catch (error) {
            throw new AIError(error.message, 'mistral', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.stream({
                model: options.model || 'mistral-tiny',
                messages,
                maxTokens: options.maxTokens,
                temperature: options.temperature || 0.7,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new AIError(error.message, 'mistral', error.status)
        }
    }
}

================
File: src/providers/OllamaClient.js
================
import ollama from 'ollama'
import { AIClient, AIError } from '../common/AIClient.js'

export class OllamaClient extends AIClient {
    constructor(config = {}) {
        super(config)
        this.baseUrl = config.baseUrl || process.env.OLLAMA_HOST || 'http://localhost:11434'
    }

    async chat(messages, options = {}) {
        try {
            const response = await ollama.chat({
                model: options.model || 'llama2',
                messages,
                ...options
            }, { baseUrl: this.baseUrl })
            return response.message.content
        } catch (error) {
            throw new AIError(error.message, 'ollama', error.code)
        }
    }

    async complete(prompt, options = {}) {
        try {
            const response = await ollama.generate({
                model: options.model || 'llama2',
                prompt,
                ...options
            }, { baseUrl: this.baseUrl })
            return response.response
        } catch (error) {
            throw new AIError(error.message, 'ollama', error.code)
        }
    }

    async embedding(text, options = {}) {
        try {
            const response = await ollama.embeddings({
                model: options.model || 'nomic-embed-text',
                prompt: text,
                ...options
            }, { baseUrl: this.baseUrl })
            return response.embedding
        } catch (error) {
            throw new AIError(error.message, 'ollama', error.code)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await ollama.chat({
                model: options.model || 'llama2',
                messages,
                stream: true,
                ...options
            }, { baseUrl: this.baseUrl })

            for await (const chunk of stream) {
                const content = chunk.message?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new AIError(error.message, 'ollama', error.code)
        }
    }
}

================
File: src/providers/OpenAIClient.js
================
// providers/openai.js
import OpenAI from 'openai'
import { AIClient, AIError } from '../common/AIClient.js'

export class OpenAIClient extends AIClient {
    constructor(config = {}) {
        super(config)
        this.client = new OpenAI({
            apiKey: config.apiKey || process.env.OPENAI_API_KEY,
            ...config.clientOptions
        })

        if (!this.client.apiKey) {
            throw new Error('OpenAI API key is required. Provide it in constructor or set OPENAI_API_KEY environment variable.')
        }
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.completions.create({
                model: options.model || 'gpt-4-turbo-preview',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new AIError(error.message, 'openai', error.status)
        }
    }

    async complete(prompt, options = {}) {
        try {
            const response = await this.client.completions.create({
                model: options.model || 'gpt-3.5-turbo-instruct',
                prompt,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].text
        } catch (error) {
            throw new AIError(error.message, 'openai', error.status)
        }
    }

    async embedding(text, options = {}) {
        try {
            const response = await this.client.embeddings.create({
                model: options.model || 'text-embedding-3-small',
                input: text,
                ...options
            })
            return response.data[0].embedding
        } catch (error) {
            throw new AIError(error.message, 'openai', error.status)
        }
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.completions.create({
                model: options.model || 'gpt-4-turbo-preview',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new AIError(error.message, 'openai', error.status)
        }
    }
}

================
File: src/providers/PerplexityClient.js
================
import OpenAI from 'openai'
import { AIClient, AIError } from '../common/AIClient.js'

export class PerplexityClient extends AIClient {
    constructor(config = {}) {
        super(config)
        const apiKey = config.apiKey || process.env.PERPLEXITY_API_KEY

        if (!apiKey) {
            throw new Error('Perplexity API key is required. Provide it in constructor or set PERPLEXITY_API_KEY environment variable.')
        }

        this.client = new OpenAI({
            apiKey,
            baseURL: 'https://api.perplexity.ai',
            ...config.clientOptions
        })
    }

    async chat(messages, options = {}) {
        try {
            const response = await this.client.chat.completions.create({
                model: options.model || 'pplx-7b-chat',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                ...options
            })
            return response.choices[0].message.content
        } catch (error) {
            throw new AIError(error.message, 'perplexity', error.status)
        }
    }

    async complete(prompt, options = {}) {
        return this.chat([{ role: 'user', content: prompt }], options)
    }

    async embedding(text, options = {}) {
        throw new AIError('Embeddings not supported by Perplexity', 'perplexity', 'UNSUPPORTED_OPERATION')
    }

    async stream(messages, callback, options = {}) {
        try {
            const stream = await this.client.chat.completions.create({
                model: options.model || 'pplx-7b-chat',
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens,
                stream: true,
                ...options
            })

            for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content || ''
                if (content) callback(content)
            }
        } catch (error) {
            throw new AIError(error.message, 'perplexity', error.status)
        }
    }
}

================
File: tests/helpers/reporter.js
================
import { SpecReporter } from 'jasmine-spec-reporter'

class CustomReporter {
    constructor() {
        this.specReporter = new SpecReporter({
            spec: {
                displayPending: true // Display pending (not fully implemented) specs
            }
        })
    }

    jasmineStarted() {
        this.specReporter.jasmineStarted.apply(this.specReporter, arguments)
    }

    suiteStarted() {
        this.specReporter.suiteStarted.apply(this.specReporter, arguments)
    }

    specStarted() {
        this.specReporter.specStarted.apply(this.specReporter, arguments)
    }

    specDone() {
        this.specReporter.specDone.apply(this.specReporter, arguments)
    }

    suiteDone() {
        this.specReporter.suiteDone.apply(this.specReporter, arguments)
    }

    jasmineDone() {
        this.specReporter.jasmineDone.apply(this.specReporter, arguments)
    }
}

export default CustomReporter;

/*
import { SpecReporter } from 'jasmine-spec-reporter';

jasmine.getEnv().clearReporters(); // Clear default console reporter
jasmine.getEnv().addReporter(new SpecReporter({
    spec: {
        displayPending: true // Display pending (not fully implemented) specs
    }
}));
*/

================
File: tests/integration/mcp-integration.spec.js
================
// spec/integration/mcp-integration.spec.js
import { expect } from 'chai'
import { createAIClient } from '../../src/common/ClientFactory.js'
import fs from 'fs/promises'
import path from 'path'

describe('MCP Integration Tests', () => {
    let client
    let testDir

    before(async () => {
        // Create test resources directory
        testDir = path.join(process.cwd(), 'test-resources')
        await fs.mkdir(testDir, { recursive: true })
        await fs.writeFile(
            path.join(testDir, 'test.txt'),
            'This is a test document for MCP integration.'
        )

        client = await createAIClient('openai', {
            apiKey: process.env.OPENAI_API_KEY,
            mcp: {
                resources: {
                    'test-doc': {
                        uri: `file://${path.join(testDir, 'test.txt')}`,
                        mimeType: 'text/plain'
                    }
                },
                tools: {
                    'file-reader': {
                        name: 'File Reader',
                        description: 'Reads file content',
                        execute: async (args) => {
                            const content = await fs.readFile(args.path, 'utf-8')
                            return content
                        }
                    }
                },
                prompts: {
                    'summarize': {
                        name: 'Summarize',
                        template: (ctx) => `Summarize the following text:\n\n${ctx.text}`
                    }
                }
            }
        })
    })

    after(async () => {
        await fs.rm(testDir, { recursive: true })
    })

    it('should read and process real file resources', async () => {
        const resource = await client.getResource('test-doc')
        expect(resource.uri).to.include('test.txt')

        const content = await client.executeTool('file-reader', {
            path: resource.uri.replace('file://', '')
        })
        expect(content).to.include('test document')
    })

    it('should use MCP-enhanced chat completion', async () => {
        const resource = await client.getResource('test-doc')
        const content = await client.executeTool('file-reader', {
            path: resource.uri.replace('file://', '')
        })

        const prompt = await client.renderPrompt('summarize', { text: content })
        const summary = await client.chat([{
            role: 'user',
            content: prompt
        }])

        expect(summary).to.be.a('string')
        expect(summary.length).to.be.greaterThan(0)
    })
})

================
File: tests/unit/base-client.spec.js
================
// spec/base-client.spec.js
import { expect } from 'chai';
import { AIClient, AIError } from '../src/base-client.js';

describe('AIClient', () => {
    class TestClient extends AIClient {}

    it('should prevent instantiation of abstract class', () => {
        expect(() => new AIClient()).to.throw('Cannot instantiate abstract class');
    });

    it('should allow instantiation of concrete implementations', () => {
        expect(() => new TestClient()).not.to.throw();
    });

    it('should require implementation of abstract methods', async () => {
        const client = new TestClient();
        await expect(client.chat([])).to.be.rejectedWith('Method chat() must be implemented');
        await expect(client.complete('')).to.be.rejectedWith('Method complete() must be implemented');
        await expect(client.embedding('')).to.be.rejectedWith('Method embedding() must be implemented');
        await expect(client.stream([], () => {})).to.be.rejectedWith('Method stream() must be implemented');
    });
});

describe('AIError', () => {
    it('should create error with provider and code', () => {
        const error = new AIError('Test error', 'test-provider', 'ERROR_CODE');
        expect(error.message).to.equal('Test error');
        expect(error.provider).to.equal('test-provider');
        expect(error.code).to.equal('ERROR_CODE');
    });
});

================
File: tests/unit/claude-2.spec.js
================
import { expect } from 'chai';
import { ClaudeClient } from '../../src/providers/ClaudeClient.js';

describe('Claude Client', () => {
    describe('Unit Tests', () => {
        let client;
        let mockResponse;

        beforeEach(() => {
            client = new ClaudeClient({ apiKey: 'test-key' });
            mockResponse = {
                content: [{ text: 'test response' }],
                embeddings: [[0.1, 0.2, 0.3]]
            };

            // Mock Anthropic client methods
            client.client = {
                messages: {
                    create: async () => mockResponse
                },
                embeddings: {
                    create: async () => ({ embeddings: [[0.1, 0.2, 0.3]] })
                }
            };
        });

        it('should require API key', () => {
            expect(() => new ClaudeClient()).to.throw(/API key is required/);
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([
                { role: 'user', content: 'test' }
            ]);
            expect(response).to.equal('test response');
        });

        it('should handle direct completion', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });

        it('should handle streaming', async () => {
            const chunks = [];
            const mockStream = {
                async *[Symbol.asyncIterator]() {
                    yield { delta: { text: 'Hello' } };
                    yield { delta: { text: ' World' } };
                }
            };

            client.client.messages.create = async () => mockStream;

            await client.stream(
                [{ role: 'user', content: 'test' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks).to.deep.equal(['Hello', ' World']);
        });

        it('should handle API errors', async () => {
            client.client.messages.create = async () => {
                throw new Error('API Error');
            };

            await expect(client.chat([]))
                .to.be.rejectedWith('API Error');
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.CLAUDE_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Claude integration tests - no API key');
                return;
            }
            client = new ClaudeClient({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is 2+2?' }
            ]);
            
            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('4');
        });

        it('should generate embeddings', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text');
            expect(embedding).to.be.an('array');
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Count to 3' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.include('1');
            expect(fullResponse).to.include('2');
            expect(fullResponse).to.include('3');
        });

        it('should handle model-specific parameters', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'Hello' }
            ], {
                model: 'claude-3-sonnet-20240229',
                temperature: 0.5,
                maxTokens: 100
            });

            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(0);
        });
    });
});

================
File: tests/unit/claude.spec.js
================
// spec/providers/claude.spec.js
import { expect } from 'chai';
import { ClaudeClient } from '../../src/providers/claude.js';

describe('Claude Client', () => {
    describe('Unit Tests', () => {
        let client;
        let mockResponse;

        beforeEach(() => {
            client = new ClaudeClient({ apiKey: 'test-key' });
            mockResponse = {
                data: {
                    choices: [{
                        message: { content: 'test response' }
                    }],
                    data: [{ embedding: [0.1, 0.2, 0.3] }]
                }
            };

            client.client = {
                createCompletion: async () => mockResponse,
                createEmbedding: async () => mockResponse,
                createCompletionStream: async () => ({
                    async *[Symbol.asyncIterator]() {
                        yield { choices: [{ delta: { content: 'Hello' } }] };
                    }
                })
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.CLAUDE_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Claude integration tests - no API key');
                return;
            }
            client = new ClaudeClient({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is 2+2?' }
            ]);
            
            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('4');
        });

        it('should generate embeddings', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text');
            expect(embedding).to.be.an('array');
            expect(embedding).to.have.lengthOf(1536);
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!process.env.CLAUDE_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Count to 3' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.include('1');
            expect(fullResponse).to.include('2');
            expect(fullResponse).to.include('3');
        });
    });
});

================
File: tests/unit/factory-mcp.spec.js
================
// spec/mcp/factory-mcp.spec.js
import { expect } from 'chai';
import { createAIClient } from '../../src/factory.js';

describe('MCP Factory Integration', () => {
    it('should create MCP-enabled client', async () => {
        const client = await createAIClient('openai', {
            apiKey: 'test-key',
            mcp: {
                resources: {
                    'test': {
                        uri: 'mcp:test',
                        mimeType: 'text/plain'
                    }
                }
            }
        });

        expect(client.getResource).to.be.a('function');
        expect(client.executeTool).to.be.a('function');
        expect(client.renderPrompt).to.be.a('function');

        const resource = await client.getResource('test');
        expect(resource.uri).to.equal('mcp:test');
    });

    it('should handle MCP tools with provider methods', async () => {
        const mockTool = {
            name: 'Test Tool',
            execute: async () => 'tool result'
        };

        const client = await createAIClient('openai', {
            apiKey: 'test-key',
            mcp: {
                tools: {
                    'test': mockTool
                }
            }
        });

        client.chat = async () => 'chat result';

        // Should have both MCP and provider methods
        const toolResult = await client.executeTool('test');
        const chatResult = await client.chat([]);

        expect(toolResult).to.equal('tool result');
        expect(chatResult).to.equal('chat result');
    });

    it('should initialize all provided MCP features', async () => {
        const config = {
            apiKey: 'test-key',
            mcp: {
                resources: {
                    'res1': { uri: 'mcp:res1' }
                },
                tools: {
                    'tool1': { 
                        name: 'Tool 1',
                        execute: async () => 'result'
                    }
                },
                prompts: {
                    'prompt1': {
                        name: 'Prompt 1',
                        template: (ctx) => `Test ${ctx.value}`
                    }
                }
            }
        };

        const client = await createAIClient('openai', config);
        const description = client.describe();

        expect(description.resources).to.have.lengthOf(1);
        expect(description.tools).to.have.lengthOf(1);
        expect(description.prompts).to.have.lengthOf(1);

        const promptResult = await client.renderPrompt('prompt1', { value: 'works' });
        expect(promptResult).to.equal('Test works');
    });

    it('should create normal client when MCP not requested', async () => {
        const client = await createAIClient('openai', {
            apiKey: 'test-key'
        });

        expect(client.getResource).to.be.undefined;
        expect(client.describe).to.be.undefined;
    });
});

================
File: tests/unit/factory.spec.js
================
// spec/factory.spec.js
import { expect } from 'chai';
import { createAIClient } from '../src/factory.js';
import { OpenAIClient } from '../src/providers/openai.js';
import { ClaudeClient } from '../src/providers/claude.js';

describe('AIClient Factory', () => {
    it('should create OpenAI client', () => {
        const client = createAIClient('openai', { apiKey: 'test' });
        expect(client).to.be.instanceOf(OpenAIClient);
    });

    it('should create Claude client', () => {
        const client = createAIClient('claude', { apiKey: 'test' });
        expect(client).to.be.instanceOf(ClaudeClient);
    });

    it('should throw error for unknown provider', () => {
        expect(() => createAIClient('unknown')).to.throw('Unknown AI provider: unknown');
    });

    it('should be case insensitive', () => {
        const client = createAIClient('OPENAI', { apiKey: 'test' });
        expect(client).to.be.instanceOf(OpenAIClient);
    });
});

================
File: tests/unit/groq.spec.js
================
// spec/providers/groq.spec.js
import { expect } from 'chai';
import { GroqClient } from '../../src/providers/groq.js';

describe('Groq Client', () => {
    describe('Unit Tests', () => {
        let client;
        let mockResponse;

        beforeEach(() => {
            client = new GroqClient({ apiKey: 'test-key' });
            mockResponse = {
                choices: [{
                    message: { content: 'test response' },
                    delta: { content: 'test chunk' }
                }]
            };

            client.client = {
                chat: {
                    completions: {
                        create: async () => mockResponse
                    }
                }
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should throw for embeddings', async () => {
            await expect(client.embedding('test')).to.be.rejectedWith(/not supported/);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.GROQ_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Groq integration tests - no API key');
                return;
            }
            client = new GroqClient({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.GROQ_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is quantum computing?' }
            ]);
            
            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(50);
        });

        it('should handle streaming', async function() {
            if (!process.env.GROQ_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Explain recursion briefly' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.include('recurs');
        });
    });
});

================
File: tests/unit/huggingface.spec.js
================
// spec/providers/huggingface.spec.js
import { expect } from 'chai';
import { HuggingFaceClient } from '../../src/providers/huggingface.js';

describe('HuggingFace Client', () => {
    describe('Unit Tests', () => {
        let client;
        let mockResponse;

        beforeEach(() => {
            client = new HuggingFaceClient({ apiKey: 'test-key' });
            mockResponse = {
                generated_text: 'test response',
                embeddings: [[0.1, 0.2, 0.3]]
            };

            client.client = {
                textGeneration: async () => mockResponse,
                featureExtraction: async () => [[0.1, 0.2, 0.3]]
            };
        });

        it('should handle text generation', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });

        it('should throw for streaming', async () => {
            await expect(client.stream([])).to.be.rejectedWith(/not supported/);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.HUGGINGFACE_API_KEY;
            if (!apiKey) {
                console.warn('Skipping HuggingFace integration tests - no API key');
                return;
            }
            client = new HuggingFaceClient({ apiKey });
        });

        it('should perform text completion', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.complete('The capital of France is');
            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('paris');
        });

        it('should generate embeddings', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text', {
                model: 'sentence-transformers/all-MiniLM-L6-v2'
            });
            
            expect(embedding).to.be.an('array');
            expect(embedding.length).to.equal(384); // MiniLM dimension
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle chat-like interactions', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is machine learning?' }
            ]);
            
            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(20);
        });

        it('should handle model-specific parameters', async function() {
            if (!process.env.HUGGINGFACE_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.complete('Once upon a time', {
                model: 'gpt2',
                parameters: {
                    do_sample: true,
                    max_new_tokens: 50,
                    temperature: 0.7
                }
            });

            expect(response).to.be.a('string');
            expect(response.length).to.be.greaterThan(10);
        });
    });
});

================
File: tests/unit/mcp-client.spec.js
================
// spec/mcp/mcp-client.spec.js
import { expect } from 'chai';
import { MCPClient } from '../../src/mcp/mcp-client.js';
import { AIError } from '../../src/base-client.js';

describe('MCP Client', () => {
    let client;

    beforeEach(() => {
        client = new MCPClient();
    });

    describe('Resource Management', () => {
        it('should register and retrieve resources', async () => {
            await client.registerResource('test', {
                uri: 'mcp:test',
                mimeType: 'text/plain'
            });

            const resource = await client.getResource('test');
            expect(resource.uri).to.equal('mcp:test');
            expect(resource.mimeType).to.equal('text/plain');
        });

        it('should generate default URI if not provided', async () => {
            await client.registerResource('test', {});
            const resource = await client.getResource('test');
            expect(resource.uri).to.equal('mcp:resource:test');
        });

        it('should throw on missing resource', async () => {
            await expect(client.getResource('nonexistent'))
                .to.be.rejectedWith(AIError)
                .and.have.property('code', 'RESOURCE_NOT_FOUND');
        });
    });

    describe('Tool Management', () => {
        it('should register and execute tools', async () => {
            const tool = {
                name: 'Test Tool',
                description: 'A test tool',
                execute: async (args) => args.value * 2
            };

            await client.registerTool('multiply', tool);
            const result = await client.executeTool('multiply', { value: 5 });
            expect(result).to.equal(10);
        });

        it('should throw on non-executable tool', async () => {
            await client.registerTool('broken', { name: 'Broken Tool' });
            await expect(client.executeTool('broken'))
                .to.be.rejectedWith(AIError)
                .and.have.property('code', 'TOOL_NOT_EXECUTABLE');
        });
    });

    describe('Prompt Management', () => {
        it('should register and render prompts', async () => {
            const prompt = {
                name: 'Test Prompt',
                template: (ctx) => `Hello ${ctx.name}!`
            };

            await client.registerPrompt('greeting', prompt);
            const result = await client.renderPrompt('greeting', { name: 'World' });
            expect(result).to.equal('Hello World!');
        });

        it('should throw on invalid prompt', async () => {
            await client.registerPrompt('invalid', { name: 'Invalid Prompt' });
            await expect(client.renderPrompt('invalid'))
                .to.be.rejectedWith(AIError)
                .and.have.property('code', 'INVALID_PROMPT');
        });
    });

    describe('MCP Description', () => {
        it('should describe all registered capabilities', async () => {
            await client.registerResource('res1', { mimeType: 'text/plain' });
            await client.registerTool('tool1', { name: 'Tool 1', description: 'Test tool' });
            await client.registerPrompt('prompt1', { name: 'Prompt 1', description: 'Test prompt' });

            const description = client.describe();
            
            expect(description.resources).to.have.lengthOf(1);
            expect(description.tools).to.have.lengthOf(1);
            expect(description.prompts).to.have.lengthOf(1);
            
            expect(description.resources[0].id).to.equal('res1');
            expect(description.tools[0].name).to.equal('Tool 1');
            expect(description.prompts[0].name).to.equal('Prompt 1');
        });
    });
});

================
File: tests/unit/mcp-persistence.spec.js
================
// spec/integration/mcp-persistence.spec.js
import { expect } from 'chai';
import { createAIClient } from '../../src/factory.js';
import fs from 'fs/promises';
import path from 'path';

describe('MCP State Persistence', () => {
    const stateFile = path.join(process.cwd(), 'mcp-state.json');
    let client;

    const persistState = async (state) => {
        await fs.writeFile(stateFile, JSON.stringify(state, null, 2));
    };

    const loadState = async () => {
        const data = await fs.readFile(stateFile, 'utf-8');
        return JSON.parse(data);
    };

    beforeEach(async () => {
        // Initial state
        const initialState = {
            resources: {
                'persisted-doc': {
                    uri: 'mcp:test',
                    mimeType: 'text/plain',
                    content: 'Persisted content'
                }
            },
            tools: {
                'persisted-tool': {
                    name: 'Persisted Tool',
                    description: 'A tool that persists',
                    results: []
                }
            }
        };
        await persistState(initialState);

        client = await createAIClient('openai', {
            apiKey: process.env.OPENAI_API_KEY,
            mcp: {
                stateFile,
                onStateChange: async (newState) => {
                    await persistState(newState);
                }
            }
        });
    });

    afterEach(async () => {
        try {
            await fs.unlink(stateFile);
        } catch (err) {
            // Ignore if file doesn't exist
        }
    });

    it('should load persisted resources', async () => {
        const resource = await client.getResource('persisted-doc');
        expect(resource.uri).to.equal('mcp:test');
        expect(resource.content).to.equal('Persisted content');
    });

    it('should persist new resources', async () => {
        await client.registerResource('new-doc', {
            uri: 'mcp:new',
            content: 'New content'
        });

        const state = await loadState();
        expect(state.resources['new-doc']).to.exist;
        expect(state.resources['new-doc'].content).to.equal('New content');
    });

    it('should track tool execution history', async () => {
        const toolState = {
            name: 'History Tool',
            description: 'Tracks execution history',
            execute: async (args) => {
                const state = await loadState();
                const tool = state.tools['history-tool'];
                tool.results.push(args);
                await persistState(state);
                return args.value * 2;
            },
            results: []
        };

        await client.registerTool('history-tool', toolState);
        await client.executeTool('history-tool', { value: 5 });

        const state = await loadState();
        expect(state.tools['history-tool'].results).to.have.lengthOf(1);
        expect(state.tools['history-tool'].results[0].value).to.equal(5);
    });

    it('should restore complete MCP state on restart', async () => {
        // Register new items
        await client.registerResource('test1', { uri: 'mcp:test1' });
        await client.registerTool('tool1', { name: 'Tool 1' });
        await client.registerPrompt('prompt1', { name: 'Prompt 1' });

        // Create new client instance
        const newClient = await createAIClient('openai', {
            apiKey: process.env.OPENAI_API_KEY,
            mcp: { stateFile }
        });

        const description = newClient.describe();
        expect(description.resources).to.have.lengthOf(2); // includes persisted-doc
        expect(description.tools).to.have.lengthOf(2);     // includes persisted-tool
        expect(description.prompts).to.have.lengthOf(1);
    });
});

================
File: tests/unit/mistral.spec.js
================
// spec/providers/mistral.spec.js
import { expect } from 'chai';
import { MistralAIClient } from '../../src/providers/mistral.js';

describe('Mistral Client', () => {
    describe('Unit Tests', () => {
        let client;
        let mockResponse;

        beforeEach(() => {
            client = new MistralAIClient({ apiKey: 'test-key' });
            mockResponse = {
                choices: [{
                    message: { content: 'test response' }
                }],
                data: [{ embedding: [0.1, 0.2, 0.3] }]
            };

            client.client = {
                chat: {
                    create: async () => mockResponse,
                    stream: async () => ({
                        async *[Symbol.asyncIterator]() {
                            yield { choices: [{ delta: { content: 'Hello' } }] };
                        }
                    })
                },
                embeddings: {
                    create: async () => mockResponse
                }
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.MISTRAL_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Mistral integration tests - no API key');
                return;
            }
            client = new MistralAIClient({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is the capital of France?' }
            ]);
            
            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('paris');
        });

        it('should generate embeddings', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text');
            expect(embedding).to.be.an('array');
            expect(embedding).to.have.length.greaterThan(0);
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Tell me a short joke' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.be.a('string').and.to.have.length.greaterThan(10);
        });

        it('should handle errors gracefully', async function() {
            if (!process.env.MISTRAL_API_KEY) {
                this.skip();
                return;
            }

            const badClient = new MistralAIClient({ apiKey: 'invalid-key' });
            await expect(badClient.chat([
                { role: 'user', content: 'test' }
            ])).to.be.rejected;
        });
    });
});

================
File: tests/unit/ollama.spec.js
================
// spec/providers/ollama.spec.js
import { expect } from 'chai';
import { OllamaClient } from '../../src/providers/ollama.js';

describe('Ollama Client', () => {
    describe('Unit Tests', () => {
        let client;
        let mockOllama;

        beforeEach(() => {
            client = new OllamaClient({ baseUrl: 'http://localhost:11434' });
            mockOllama = {
                chat: async () => ({ message: { content: 'test response' } }),
                generate: async () => ({ response: 'test completion' }),
                embeddings: async () => ({ embedding: [0.1, 0.2, 0.3] })
            };
            global.ollama = mockOllama;
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle direct completion', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test completion');
        });

        it('should handle embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(async () => {
            // Check if Ollama is running
            try {
                const response = await fetch('http://localhost:11434/api/version');
                if (!response.ok) {
                    console.warn('Skipping Ollama integration tests - service not running');
                    return;
                }
            } catch (e) {
                console.warn('Skipping Ollama integration tests - service not available');
                return;
            }
            client = new OllamaClient({});
        });

        it('should perform chat completion', async function() {
            if (!client) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is 2+2?' }
            ], { model: 'llama2' });
            
            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('4');
        });

        it('should generate embeddings', async function() {
            if (!client) {
                this.skip();
                return;
            }

            const embedding = await client.embedding('Test text', { 
                model: 'nomic-embed-text'
            });
            
            expect(embedding).to.be.an('array');
            expect(embedding).to.have.lengthOf(768); // nomic-embed-text dimension
            embedding.forEach(value => {
                expect(value).to.be.a('number');
            });
        });

        it('should handle streaming', async function() {
            if (!client) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Count to 3' }],
                chunk => chunks.push(chunk),
                { model: 'llama2' }
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.match(/[123]/);
        });

        it('should handle model not found errors', async function() {
            if (!client) {
                this.skip();
                return;
            }

            await expect(client.chat([
                { role: 'user', content: 'test' }
            ], { model: 'nonexistent-model' })).to.be.rejectedWith(/model.*not found/i);
        });
    });
});

================
File: tests/unit/openai.spec.js
================
// spec/providers/openai.spec.js
import { expect } from 'chai';
import { OpenAIClient } from '../../src/providers/openai.js';

describe('OpenAI Client', () => {
    let client;
    let mockResponse;

    beforeEach(() => {
        client = new OpenAIClient({ apiKey: 'test-key' });
        mockResponse = {
            choices: [{
                message: { content: 'test response' },
                text: 'test completion'
            }],
            data: [{ embedding: [0.1, 0.2, 0.3] }]
        };

        // Mock OpenAI client methods
        client.client = {
            chat: {
                completions: {
                    create: async () => mockResponse
                }
            },
            completions: {
                create: async () => mockResponse
            },
            embeddings: {
                create: async () => mockResponse
            }
        };
    });

    describe('chat', () => {
        it('should return chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should handle errors', async () => {
            client.client.chat.completions.create = async () => {
                throw new Error('API Error');
            };
            await expect(client.chat([])).to.be.rejectedWith('API Error');
        });
    });

    describe('complete', () => {
        it('should return completion', async () => {
            const response = await client.complete('test prompt');
            expect(response).to.equal('test completion');
        });
    });

    describe('embedding', () => {
        it('should return embeddings', async () => {
            const response = await client.embedding('test text');
            expect(response).to.deep.equal([0.1, 0.2, 0.3]);
        });
    });

    describe('stream', () => {
        it('should handle streaming responses', async () => {
            const chunks = [];
            const mockStream = {
                async *[Symbol.asyncIterator]() {
                    yield { choices: [{ delta: { content: 'Hello' } }] };
                    yield { choices: [{ delta: { content: ' World' } }] };
                }
            };

            client.client.chat.completions.create = async () => mockStream;

            await client.stream(
                [{ role: 'user', content: 'test' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks).to.deep.equal(['Hello', ' World']);
        });
    });
});

================
File: tests/unit/perplexity.spec.js
================
// spec/providers/perplexity.spec.js
import { expect } from 'chai';
import { PerplexityClient } from '../../src/providers/perplexity.js';

describe('Perplexity Client', () => {
    describe('Unit Tests', () => {
        let client;
        let mockResponse;

        beforeEach(() => {
            client = new PerplexityClient({ apiKey: 'test-key' });
            mockResponse = {
                choices: [{
                    message: { content: 'test response' },
                    delta: { content: 'test chunk' }
                }]
            };

            client.client = {
                chat: {
                    completions: {
                        create: async () => mockResponse
                    }
                }
            };
        });

        it('should handle chat completion', async () => {
            const response = await client.chat([{ role: 'user', content: 'test' }]);
            expect(response).to.equal('test response');
        });

        it('should throw for embeddings', async () => {
            await expect(client.embedding('test')).to.be.rejectedWith(/not supported/);
        });
    });

    describe('Integration Tests', () => {
        let client;

        before(() => {
            const apiKey = process.env.PERPLEXITY_API_KEY;
            if (!apiKey) {
                console.warn('Skipping Perplexity integration tests - no API key');
                return;
            }
            client = new PerplexityClient({ apiKey });
        });

        it('should perform chat completion', async function() {
            if (!process.env.PERPLEXITY_API_KEY) {
                this.skip();
                return;
            }

            const response = await client.chat([
                { role: 'user', content: 'What is the largest moon in our solar system?' }
            ]);
            
            expect(response).to.be.a('string');
            expect(response.toLowerCase()).to.include('ganymede');
        });

        it('should handle streaming', async function() {
            if (!process.env.PERPLEXITY_API_KEY) {
                this.skip();
                return;
            }

            const chunks = [];
            await client.stream(
                [{ role: 'user', content: 'Name three planets' }],
                chunk => chunks.push(chunk)
            );

            expect(chunks.length).to.be.greaterThan(0);
            const fullResponse = chunks.join('');
            expect(fullResponse).to.match(/mercury|venus|mars|jupiter|saturn|uranus|neptune/i);
        });

        it('should handle rate limits', async function() {
            if (!process.env.PERPLEXITY_API_KEY) {
                this.skip();
                return;
            }

            const promises = Array(10).fill().map(() => 
                client.chat([{ role: 'user', content: 'test' }])
            );

            await expect(Promise.all(promises))
                .to.be.rejectedWith(/rate limit/i);
        });
    });
});

================
File: .git
================
gitdir: ../../.git/modules/packages/clients

================
File: .gitignore
================
.env


# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

================
File: clients.d.ts
================
// clients.d.ts
import { AIConfig, Message, MCPResource, MCPTool, MCPPrompt, MCPState } from './index';

export abstract class AIClient {
    constructor(config?: AIConfig);
    abstract chat(messages: Message[], options?: Record<string, any>): Promise<string>;
    abstract complete(prompt: string, options?: Record<string, any>): Promise<string>;
    abstract embedding(text: string | string[], options?: Record<string, any>): Promise<number[]>;
    abstract stream(messages: Message[], callback: (chunk: string) => void, options?: Record<string, any>): Promise<void>;
}

export class MCPClient extends AIClient {
    registerResource(id: string, resource: MCPResource): Promise<void>;
    registerTool(id: string, tool: MCPTool): Promise<void>;
    registerPrompt(id: string, prompt: MCPPrompt): Promise<void>;
    getResource(id: string): Promise<MCPResource>;
    executeTool(id: string, args: Record<string, any>): Promise<any>;
    renderPrompt(id: string, context: Record<string, any>): Promise<string>;
    describe(): MCPState;
}

export function createAIClient(
    provider: string,
    config?: AIConfig
): Promise<AIClient & Partial<MCPClient>>;

// Provider-specific types
export interface OpenAIOptions {
    model?: string;
    temperature?: number;
    maxTokens?: number;
    [key: string]: any;
}

export interface MistralOptions {
    model?: string;
    maxTokens?: number;
    temperature?: number;
    [key: string]: any;
}

export interface OllamaOptions {
    model?: string;
    parameters?: {
        temperature?: number;
        num_ctx?: number;
        [key: string]: any;
    };
}

================
File: env-example.txt
================
# .env.example
OPENAI_API_KEY=your_openai_key
CLAUDE_API_KEY=your_claude_key
MISTRAL_API_KEY=your_mistral_key
GROQ_API_KEY=your_groq_key
PERPLEXITY_API_KEY=your_perplexity_key
HUGGINGFACE_API_KEY=your_huggingface_key

# Ollama settings
OLLAMA_HOST=http://localhost:11434

# Test settings
TEST_TIMEOUT=10000

================
File: github-workflow (1).txt
================
name: Test Suite

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434

    strategy:
      matrix:
        node-version: [18.x, 20.x]

    steps:
    - uses: actions/checkout@v4
    
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Setup test environment
      run: |
        cp .env.example .env.test
        echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> .env.test
        echo "CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }}" >> .env.test
        echo "MISTRAL_API_KEY=${{ secrets.MISTRAL_API_KEY }}" >> .env.test
        echo "GROQ_API_KEY=${{ secrets.GROQ_API_KEY }}" >> .env.test
        echo "PERPLEXITY_API_KEY=${{ secrets.PERPLEXITY_API_KEY }}" >> .env.test
        echo "HUGGINGFACE_API_KEY=${{ secrets.HUGGINGFACE_API_KEY }}" >> .env.test
      
    - name: Pull Ollama models
      run: |
        curl http://localhost:11434/api/pull -d '{"name":"llama2"}'
        curl http://localhost:11434/api/pull -d '{"name":"nomic-embed-text"}'
      
    - name: Run unit tests
      run: npm test
      
    - name: Run integration tests
      if: github.event_name == 'push'
      run: NODE_ENV=test npm test -- spec/integration
      
    - name: Upload coverage
      uses: codecov/codecov-action@v4

================
File: github-workflow.txt
================
name: Test Suite

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434

    strategy:
      matrix:
        node-version: [18.x, 20.x]

    steps:
    - uses: actions/checkout@v4
    
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Setup test environment
      run: |
        cp .env.example .env.test
        echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> .env.test
        echo "CLAUDE_API_KEY=${{ secrets.CLAUDE_API_KEY }}" >> .env.test
        echo "MISTRAL_API_KEY=${{ secrets.MISTRAL_API_KEY }}" >> .env.test
        echo "GROQ_API_KEY=${{ secrets.GROQ_API_KEY }}" >> .env.test
        echo "PERPLEXITY_API_KEY=${{ secrets.PERPLEXITY_API_KEY }}" >> .env.test
        echo "HUGGINGFACE_API_KEY=${{ secrets.HUGGINGFACE_API_KEY }}" >> .env.test
      
    - name: Pull Ollama models
      run: |
        curl http://localhost:11434/api/pull -d '{"name":"llama2"}'
        curl http://localhost:11434/api/pull -d '{"name":"nomic-embed-text"}'
      
    - name: Run unit tests
      run: npm test
      
    - name: Run integration tests
      if: github.event_name == 'push'
      run: NODE_ENV=test npm test -- spec/integration
      
    - name: Upload coverage
      uses: codecov/codecov-action@v4

================
File: index.d.ts.ts
================
// index.d.ts
export interface AIConfig {
    apiKey?: string;
    clientOptions?: Record<string, any>;
    mcp?: MCPConfig;
}

export interface MCPConfig {
    resources?: Record<string, MCPResource>;
    tools?: Record<string, MCPTool>;
    prompts?: Record<string, MCPPrompt>;
    stateFile?: string;
    onStateChange?: (state: MCPState) => Promise<void>;
}

export interface MCPResource {
    uri?: string;
    mimeType?: string;
    [key: string]: any;
}

export interface MCPTool {
    name: string;
    description?: string;
    execute: (args: Record<string, any>) => Promise<any>;
}

export interface MCPPrompt {
    name: string;
    description?: string;
    template: (context: Record<string, any>) => string;
}

export interface MCPState {
    resources: Record<string, MCPResource>;
    tools: Record<string, MCPTool>;
    prompts: Record<string, MCPPrompt>;
}

export interface Message {
    role: 'user' | 'assistant' | 'system';
    content: string;
}

export class AIError extends Error {
    constructor(message: string, provider: string, code: string);
    provider: string;
    code: string;
}

================
File: jasmine.json
================
{
  "spec_dir": "tests",
  "spec_files": [
    "**/*[sS]pec.?(m)js"
  ],
  "helpers": [
    "helpers/**/*.?(m)js"
  ],
  "env": {
    "stopSpecOnExpectationFailure": false,
    "random": true
  }
}

================
File: package.json
================
{
  "name": "hyperdata-clients",
  "version": "1.0.0",
  "type": "module",
  "description": "Unified client library for multiple AI providers",
  "main": "src/factory.js",
  "scripts": {
    "coverage": "NODE_ENV=test c8 npm test",
    "test": "jasmine --config=jasmine.json --reporter=tests/helpers/reporter.js",
    "docs": "jsdoc -c jsdoc.json",
    "rp": "repomix -c repomix.config.json . "
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.17.1",
    "@huggingface/inference": "^2.6.4",
    "@mistralai/mistralai": "^0.0.10",
    "dotenv": "^16.4.1",
    "groq-sdk": "^0.3.0",
    "ollama": "^0.4.4",
    "openai": "^4.28.0"
  },
  "devDependencies": {
    "c8": "^9.1.0",
    "chai": "^5.0.3",
    "jasmine": "^5.5.0",
    "jasmine-spec-reporter": "^7.0.0",
    "jsdoc": "^4.0.4"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}

================
File: readme.md
================
# AI Client Library

Unified client library for interacting with multiple AI providers including OpenAI, Claude, Mistral, Groq, Perplexity, HuggingFace and Ollama.

## Installation

```bash
npm install ai-client-library
```

## Usage

```javascript
import { createAIClient } from 'ai-client-library';

const client = createAIClient('openai', { 
  apiKey: 'your-api-key' 
});

// Chat completion
const response = await client.chat([
  { role: 'user', content: 'Hello!' }
]);

// Stream response
await client.stream(
  [{ role: 'user', content: 'Tell me a story' }],
  chunk => console.log(chunk)
);
```

## Supported Providers

- OpenAI
- Claude (Anthropic)
- Ollama
- Mistral
- Groq
- Perplexity
- HuggingFace

## Development

```bash
npm install
npm test
```

## License

MIT

================
File: README.md
================
# clients
some API clients

================
File: repomix.config.json
================
{
    "output": {
        "filePath": "./repomix-clients.md",
        "headerText": "clients repo",
        "removeComments": false
    },
    "include": [
        "**/*"
    ],
    "ignore": {
        "useDefaultPatterns": false,
        "customPatterns": [
            "docs",
            "data",
            "docs/jsdoc",
            ".nyc_output",
            ".env",
            "**/_*",
            "node_modules",
            "*.log",
            "**/*repomix*.txt",
            "**/*.html",
            "**/data/*",
            "**/*copy.js",
            "**/conversations.json"
        ]
    }
}

================
File: test-env.txt
================
# .env.test
OPENAI_API_KEY=test_key
CLAUDE_API_KEY=test_key
MISTRAL_API_KEY=test_key
GROQ_API_KEY=test_key
PERPLEXITY_API_KEY=test_key
HUGGINGFACE_API_KEY=test_key
OLLAMA_HOST=http://localhost:11434
TEST_TIMEOUT=1000

================
File: types-package.json
================
// package.json
{
  "name": "@types/ai-client-library",
  "version": "1.0.0",
  "types": "index.d.ts",
  "dependencies": {
    "@types/node": "^20.0.0"
  }
}
